{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[1] https://medium.com/deep-learning-turkey/text-processing-1-old-fashioned-methods-bag-of-words-and-tfxidf-b2340cc7ad4b, Medium, Deniz Kilinc visited 6th of April 2019\\n[2] https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm, from ReiiNakano , Kaggle, visited 5th of April 2019\\n[3] https://github.com/codebasics/py/tree/master/ML, github, Codebasics from dhavalsays, visited 6th of April 2019\\n[4] from scikit-learn.org (base code), visited 4th of April 2019\\n[5] Python One Hot Encoding with SciKit Learn, InsightBot, http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn, visited 6th April 2019 \\n[6] Kaggle, Sentiment Analysis : CountVectorizer & TF-IDF, Divyojyoti Sinha, https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\\n[7] Kaggle, Bert Carremans, Using Word Embeddings for Sentiment Analysis, https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis, visited april 11th 2019\\n[8] Sentiment Analysis with pretrained Word2Vec, Varun Sharma, Kaggle, https://www.kaggle.com/varunsharmaml/sentiment-analysis-with-pretrained-word2vec, visited 12th of april 2019\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from IPython import get_ipython\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    ipy.run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import re\n",
    "import io\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim\n",
    "import keras.backend as K\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot.plotters as skplt\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Activation, LeakyReLU, PReLU, ELU, ThresholdedReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import unit_norm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from gensim.models import FastText\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Conv1D, MaxPooling1D, Dropout, Concatenate, Conv2D, MaxPooling2D, concatenate,BatchNormalization, Bidirectional\n",
    "from keras.initializers import glorot_uniform\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.activations import relu\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import seaborn as sns\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from gensim.models.wrappers import FastText\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import datetime\n",
    "import fastText\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import string\n",
    "import operator \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "num_partitions = multiprocessing.cpu_count()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "#Global Variables from [7] and others\n",
    "NB_WORDS = 9501  # Parameter indicating the number of words we'll put in the dictionary \n",
    "\n",
    "VAL_SIZE = 9  # Size of the validation set (originally 1000)\n",
    "NB_START_EPOCHS = 8  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 72  # Maximum number of words in a sequence\n",
    "#MAX_LEN = 62\n",
    "GLOVE_DIM = 50  # Number of dimensions of the GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 456\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "#activation function\n",
    "act = 'relu'\n",
    "re_weight = True\n",
    "#dimension for fasttext\n",
    "DIM = 300\n",
    "#dimension for word2vec\n",
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "dim = 50\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "maxLen = 60\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "lstm_output_size = 70\n",
    "#model2 = fastText.train_supervised('./fasttext_train.txt',label='label_', epoch=20, dim=200)\n",
    "\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\s\\W',' ',s)\n",
    "    s = re.sub(r'\\W,\\s',' ',s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(r\"\\d+\", \"\", s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\",\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    \n",
    "    return s\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode('utf8', 'ignore')\n",
    "    return txt\n",
    "\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    data['FreeText'] = [cleaning(s) for s in data['FreeText']]\n",
    "    corpus = []\n",
    "    for col in ['FreeText']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "    \n",
    "    with open('corpus.pickle', 'wb') as handle:\n",
    "        pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return corpus\n",
    "def tsne_plot(df):\n",
    "    corpus = build_corpus(df)\n",
    "    \n",
    "    model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=1, workers=4)\n",
    "    with open('w2vmodel.pickle', 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #model.most_similar('')\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "def transformAge(df):\n",
    "    '''\n",
    "    This function will categorise the age data into 5 different age categories\n",
    "    '''\n",
    "    dfage = df.Age\n",
    "\n",
    "    array = []\n",
    "    for i in range(len(dfage)):\n",
    "        if dfage[i] < 3:\n",
    "            array.append(0)\n",
    "        elif dfage[i] >= 3 and dfage[i] < 13:\n",
    "            array.append(1)\n",
    "        elif dfage[i] >= 13 and dfage[i] < 19:\n",
    "            array.append(2)\n",
    "        elif dfage[i] >= 19 and dfage[i] < 65:\n",
    "            array.append(3)\n",
    "        else:\n",
    "            array.append(4)\n",
    "    df[\"AgeCat\"] = array\n",
    "    return df\n",
    "\n",
    "def transformLCD(df):\n",
    "    '''\n",
    "    This function will transform the numerical LCD data into 3 categories\n",
    "    '''\n",
    "    dflcd = df.LastContactDays\n",
    "    array = []\n",
    "    for i in range(len(dflcd)):\n",
    "        if dflcd[i] < 2:\n",
    "            array.append(0)\n",
    "        elif dflcd[i] >= 2 and dflcd[i] < 31:\n",
    "            array.append(1)\n",
    "        else:\n",
    "            array.append(2)\n",
    "    df[\"LcdCat\"] = array\n",
    "    return df\n",
    "\n",
    "def test_code(input):\n",
    "    print(\"Hello \" + input)\n",
    "\n",
    "def eda1(df):\n",
    "    figsize=(20, 10)\n",
    "    \n",
    "    ticksize = 14\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "            'axes.labelsize' : labelsize,\n",
    "            'axes.titlesize' : titlesize,\n",
    "            'xtick.labelsize': ticksize,\n",
    "            'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "    \n",
    "    plt.subplot(441)\n",
    "    x1=df['prio']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x1)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Priority')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "\n",
    "    plt.subplot(442)\n",
    "    x2=df['operator']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x2)\n",
    "   # plt.title(\"Review Sentiment Count\")\n",
    "    plt.title('Operator')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.plot()\n",
    "    \n",
    "    #pout\n",
    "    plt.subplot(443)\n",
    "    x3=df['pout']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x3)\n",
    "    plt.title('Pout')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(444)\n",
    "    x4=df['hosp_ed']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Hospitaled')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(449)\n",
    "    x4=df['AgeCat']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Age')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,10)\n",
    "    x4=df['Gender']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Gender')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,11)\n",
    "    x4=df['LastContactN']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('LastContactN')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,12)\n",
    "    x4=df['LcdCat']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('LastContactDays')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "def eda2(df):\n",
    "    #max number of words in a sentence\n",
    "    df1 = pd.DataFrame(df)\n",
    "    df1['FreeText_count'] = df['FreeText'].apply(lambda x: Counter(x.split(' ')))\n",
    "    LEN = df1['FreeText_count'].apply(lambda x : sum(x.values()))\n",
    "    max_LEN = max(LEN)\n",
    "    global MAX_LEN\n",
    "    MAX_LEN = max_LEN\n",
    "    #length of sequence\n",
    "    df[\"FreeText_len\"] = df[\"FreeText\"].apply(lambda x: len(x))\n",
    "    #maximum number of sequence length\n",
    "    #print(df[\"FreeText_len\"].max())\n",
    "    df[\"FreeText_len\"].hist(figsize = (15, 10), bins = 100)\n",
    "    plt.show()\n",
    "    global MAX_SEQUENCE_LENGTH\n",
    "    MAX_SEQUENCE_LENGTH = df[\"FreeText_len\"].max()\n",
    "    global NB_WORDS\n",
    "    NB_WORDS = df['FreeText_len'].sum()\n",
    "    #word EDA\n",
    "    dummies2 = df.iloc[:,1:2]\n",
    "    tsne_plot(dummies2)\n",
    "    #wordcloud\n",
    "    df['FreeText'] = [cleaning(s) for s in df['FreeText']]\n",
    "    input_data = df['FreeText']\n",
    "    word_cloud(input_data)\n",
    "    with open('corpus.pickle', 'rb') as handle:\n",
    "        corpus = pickle.load(handle)\n",
    "    xt = np.concatenate(corpus)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    pd.value_counts(xt).plot(kind=\"barh\")\n",
    "    plt.show()\n",
    "    \n",
    "def word_cloud(input_data,title=None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 200,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(input_data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "#preprocessing function. use the dummies as input\n",
    "def pre_processing1(input,df):\n",
    "    sentence = [None] * df.shape[0]\n",
    "    for i in range(len(sentence)):\n",
    "        old_sentence = input.iloc[i]\n",
    "        word = list(old_sentence.split())\n",
    "        words = [None] * len(word)\n",
    "        for i in range(len(word)):\n",
    "            words[i] = re.sub(r'\\W+', '', word[i].lower())\n",
    "        words1 = [x for x in words if x is not None]\n",
    "        sentence.append(' '.join(words1))\n",
    "        sentence1 = [x for x in sentence if x is not None]\n",
    "    values = array(sentence1)\n",
    "    return values\n",
    "\n",
    "#this methods is courtesy from [1], it is an alternative preprocessing method that also uses stop words removal and normalization (in the main section)\n",
    "def pre_processing2(input):\n",
    "    #np.vectorize(input)\n",
    "    dummies1=re.sub(r\"\\w+\", \" \", input)\n",
    "    pattern = r\"[{}]\".format(\",.;\")\n",
    "    dummies1=re.sub(pattern, \"\", input)\n",
    "    #lower casing\n",
    "    dummies1= dummies1.lower()\n",
    "    dummies1 = dummies1.strip()\n",
    "    WPT = nltk.WordPunctTokenizer()\n",
    "    #tokenization\n",
    "    tokens = WPT.tokenize(dummies1)\n",
    "    #stop words removal\n",
    "    stop_word_list = nltk.corpus.stopwords.words('swedish')\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_word_list]\n",
    "    result = ' '.join(filtered_tokens)\n",
    "    return result\n",
    "\n",
    "#this function is used to transform string data into float data: e.g. Pout (String) to NewPout (float) using a scoring method where the highest value is the highest priority\n",
    "def transform_output_data(output_dataframe,datatype=None):\n",
    "    \n",
    "    #alternative 1\n",
    "    data2 = [None] * output_dataframe.shape[0]\n",
    "    data = output_dataframe\n",
    "    #float datatype by default\n",
    "    if datatype is None:\n",
    "        for i in range(len(data2)):\n",
    "            if data[i] == '1A':\n",
    "                data2.append(1.0)\n",
    "            elif data[i] == '1B':\n",
    "                data2.append(0.8)\n",
    "            elif data[i] == '2A':\n",
    "                data2.append(0.6)\n",
    "            elif data[i] == '2B':\n",
    "                data2.append(0.4)\n",
    "            else:\n",
    "                data2.append(0.2)\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "        data2 = np.array(data1)\n",
    "        df_data = pd.DataFrame({'NewPout': data2})\n",
    "        return df_data\n",
    "    elif datatype == 'int':\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == '1A':\n",
    "                data2.append(1)\n",
    "            elif data[i] == '1B':\n",
    "                data2.append(2)\n",
    "            elif data[i] == '2A':\n",
    "                data2.append(3)\n",
    "            elif data[i] == '2B':\n",
    "                data2.append(4)\n",
    "            else:\n",
    "                data2.append(5)\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "        data2 = np.array(data1)\n",
    "        df_data = pd.DataFrame({'NewPout': data2})\n",
    "        return df_data\n",
    "    elif datatype == 'multi':\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(output_dataframe)\n",
    "        label_encoded_y = label_encoder.transform(output_dataframe)\n",
    "        df_data = pd.DataFrame({'NewPout': label_encoded_y})\n",
    "        return df_data\n",
    "    #note to self: don't binarize the outputs, if there is binary outputs: use BoW for the binary outputs\n",
    "def inverse_transform_output_data(input):\n",
    "    data2 = [None] * input.shape[0]\n",
    "    data = input\n",
    "    for i in range(len(data2)):\n",
    "        if data[i] == 1.0 or data[i] == 1:\n",
    "            data2.append(\"1A\")\n",
    "        elif data[i] == 0.8 or data[i] == 2:\n",
    "            data2.append(\"1B\")\n",
    "        elif data[i] == 0.6 or data[i] == 3:\n",
    "            data2.append(\"2A\")\n",
    "        elif data[i] == 0.4 or data[i] == 4:\n",
    "            data2.append(\"2B\")\n",
    "        else:\n",
    "            data2.append(\"Referral\")\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "    data2 = np.array(data1)\n",
    "    df_data = pd.DataFrame({'NewPout': data2})\n",
    "    return df_data\n",
    "def text_processing(input_data, output_data, processing_method=None, truncated=None,datatype=None):\n",
    "    #bag-of-words for the none clause\n",
    "    if processing_method == None:\n",
    "        #one of these alternatives can be used but it depends on the classification result\n",
    "        #[2]\n",
    "        #Alternative 1 from [2]\n",
    "        #try to use aside from word: char or char_wb\n",
    "        bag_of_words_vector = CountVectorizer(analyzer=\"word\")\n",
    "        bag_of_words_matrix = bag_of_words_vector.fit_transform(input_data)\n",
    "        #denna är viktig\n",
    "        bag_of_words_matrix = bag_of_words_matrix.toarray()\n",
    "        '''\n",
    "        #Alternative 2\n",
    "        bag_of_words_vector = CountVectorizer(min_df = 0.0, max_df = 1.0, ngram_range=(2,2))\n",
    "        bag_of_words_matrix = bag_of_words_vector.fit_transform(input_data)\n",
    "        #denna är viktig\n",
    "        bag_of_words_matrix = bag_of_words_matrix.toarray()\n",
    "        '''\n",
    "        #using LSA: Latent Semantic Analysis or LSI\n",
    "        if truncated == 1:\n",
    "            svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "            truncated_bag_of_words = svd.fit_transform(bag_of_words_matrix)\n",
    "            #you can swap bag_of_words with truncated_bag_of_words\n",
    "            result = feature_engineering(truncated_bag_of_words,output_data)\n",
    "        result = feature_engineering(bag_of_words_matrix,output_data)\n",
    "        return result\n",
    "    elif processing_method =='tfidf':\n",
    "        \n",
    "        #[2]\n",
    "        Tfidf_Vector = TfidfVectorizer(analyzer=\"char_wb\")    \n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        '''\n",
    "        #Alternative 2 from [1]\n",
    "        Tfidf_Vector = TfidfVectorizer(min_df = 0., max_df = 1., use_idf = True)\n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        \n",
    "        \n",
    "        #Alternative 3\n",
    "        Tfidf_Vector = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1,1), sublinear_tf=True)\n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        '''\n",
    "       \n",
    "        \n",
    "        if truncated == 1:\n",
    "            svd2 = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "            #do we need to truncate the matrix?\n",
    "            #do we need to transform Tfidf_Matrix to an array before truncation?\n",
    "            truncated_tfidf = svd2.fit_transform(Tfidf_Matrix)\n",
    "            result = feature_engineering(truncated_tfidf,output_data)\n",
    "        #try to use truncated_tfidf instead tfidf_Matrix to see what happens\n",
    "        result = feature_engineering(Tfidf_Matrix,output_data)\n",
    "        return result\n",
    "    elif processing_method == 'onehot':\n",
    "        #be warned: one hot encoding only work well with binary outputs\n",
    "        #originates from [3]\n",
    "        label_encoder_input = LabelEncoder()\n",
    "        #label_encoder_output = LabelEncoder()\n",
    "        print(output_data.shape)\n",
    "        output1 = output_data.to_numpy()\n",
    "        array1 = [None] * input_data.shape[0]\n",
    "        for i in range(len(array1)):\n",
    "            input = input_data[i].split()\n",
    "            \n",
    "            values = array(input)\n",
    "            values1 = [x for x in values if x is not None]\n",
    "            array1.append(values1)\n",
    "        array2 = [x for x in array1 if x is not None]\n",
    "        array3 = array(array2)\n",
    "        array4 = np.hstack(array3)\n",
    "        array4.reshape(-1,len(output1.shape))\n",
    "        #output1 = output1.reshape(array4.shape)\n",
    "        #print(array4)\n",
    "        \n",
    "        integer_encoded_input = label_encoder_input.fit_transform(array4)\n",
    "        \n",
    "        #integer_encoded_output = label_encoder_output.fit_transform(output_data)\n",
    "        #float by default\n",
    "        if datatype is None:\n",
    "            #this method performs one hot encoding to return data of type float\n",
    "            onehot_encoder_input = OneHotEncoder(sparse=False)\n",
    "\n",
    "            #using reshaping before encoding\n",
    "            integer_encoded_input = integer_encoded_input.reshape(-1, 1)\n",
    "            encoded_input = onehot_encoder_input.fit_transform(integer_encoded_input)\n",
    "            \n",
    "            output= transform_output_data(output_data,'multi')\n",
    "            output1 = output.to_numpy()\n",
    "            \n",
    "           \n",
    "            #encoded_output = onehot_encoder_output.fit_transform(integer_encoded_output)\n",
    "        if datatype == 'int':\n",
    "            input_lb = LabelBinarizer()\n",
    "            encoded_input = input_lb.fit_transform(integer_encoded_input)\n",
    "            print(encoded_input)\n",
    "            print(encoded_input.shape)\n",
    "        #create training and test data using our encoded data\n",
    "        #change from integer_encoded_output to output_data\n",
    "        result = feature_engineering(encoded_input,output1)\n",
    "        \n",
    "        return result\n",
    "#split data into train and test data\n",
    "def feature_engineering(input_data, output_data):\n",
    "    #alternative 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.3, random_state=37)\n",
    "    '''\n",
    "    #alternative 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.3)\n",
    "    '''\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=37)\n",
    "    '''\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    result = [X_train, X_test, y_train, y_test]\n",
    "    return result\n",
    "def predictor(data_array,method,multiclass):\n",
    "    if method == 'NBG':\n",
    "        NBGres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        NBGres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [NBGres_BoW,NBGres_tfidf]\n",
    "        return result\n",
    "    elif method == 'NBM':\n",
    "        NBMres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        NBMres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [NBMres_BoW,NBMres_tfidf]\n",
    "        return result\n",
    "    elif method == 'SVM':\n",
    "        SVMres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        SVMres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [SVMres_BoW,SVMres_tfidf]\n",
    "        return result\n",
    "    elif method == 'RF':\n",
    "        RFres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        RFres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [RFres_BoW,RFres_tfidf]\n",
    "        return result\n",
    "    elif method == 'ensemble':\n",
    "        res_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        res_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [res_BoW,res_tfidf]\n",
    "        return result\n",
    "    elif method == 'GB':\n",
    "        GBres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        GBres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [GBres_BoW,GBres_tfidf]\n",
    "        return result\n",
    "    else:\n",
    "        logres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        logres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [logres_BoW,logres_tfidf]\n",
    "        #pred_prob is at index 4\n",
    "    return result\n",
    "#[6] prediction using the processed data\n",
    "def generate_metrics(result,clf=None):\n",
    "    title = ['Bow','TFIDF']\n",
    "    if clf == 'NBG':\n",
    "        val = 'Naive_Bayes_Gaussian'\n",
    "    elif clf == 'NBM':\n",
    "        val = 'Naive_Bayes_Multinomial'\n",
    "    elif clf == 'SVM':\n",
    "        val = 'Support_Vector_Machine'\n",
    "    elif clf == 'RF':\n",
    "        val = 'Random_Forest'\n",
    "    elif clf == 'ensemble':\n",
    "        val = 'Ensemble'\n",
    "    elif clf == 'gb':\n",
    "        val = 'Gradient_Boosting'\n",
    "    else:\n",
    "        val = 'Logistic_Regression'\n",
    "    print('Metrics from Bag of Words on '+ val +':')\n",
    "    print('-'*30)\n",
    "    result_from_predicitions(result[0])\n",
    "    print('-'*30)\n",
    "    print('Metrics from TF-IDF on '+ val +':')\n",
    "    print('-'*30)\n",
    "    result_from_predicitions(result[1])\n",
    "    print('-'*200)\n",
    "    plot_classification_report(result[0],result[1],title,val)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*200)\n",
    "    cm1 = metrics.confusion_matrix(y_true=result[0][0], y_pred=result[0][1])\n",
    "    print(cm1)\n",
    "    print('-'*200)\n",
    "    cm2 = metrics.confusion_matrix(y_true=result[1][0], y_pred=result[1][1])\n",
    "    print(cm2)\n",
    "    plot_cm(cm1,cm2,val)\n",
    "    print('-'*200)\n",
    "def train_predict_model(classifier,X_train,X_test, y_train, y_test,multiclass):\n",
    "    # build model\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    if classifier == 'NBG':\n",
    "        model = GaussianNB()\n",
    "    elif classifier == 'NBM':\n",
    "        model = MultinomialNB()\n",
    "    elif classifier == 'SVM':\n",
    "        model = LinearSVC()\n",
    "    elif classifier == 'RF':\n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "    elif classifier == 'ensemble':\n",
    "        model1 = LogisticRegression()\n",
    "        model2 = MultinomialNB()\n",
    "        model3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "        model = VotingClassifier(estimators=[('lr', model1), ('nb', model2), ('rf', model3)], voting='hard')\n",
    "    elif classifier == 'GB':\n",
    "        model = XGBClassifier(n_estimators=100)\n",
    "    else:\n",
    "        if multiclass == 'yes':\n",
    "            model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
    "        else:\n",
    "            model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    # predict using model\n",
    "    predicted = model.predict(X_test)\n",
    "    if (classifier != 'SVM' or classifier != 'ensemble'):\n",
    "        pred_prob = model.predict_proba(X_test)[:,1]\n",
    "        #pred_prob = None\n",
    "    else:\n",
    "        pred_prob = None\n",
    "    acc = metrics.accuracy_score(y_test,predicted)\n",
    "    acc = acc*100\n",
    "    if classifier == None:\n",
    "        loss = log_loss(y_test,predicted)\n",
    "        result = [predicted,acc,loss,pred_prob]\n",
    "    else:\n",
    "    \n",
    "        result = [predicted,acc,pred_prob]\n",
    "    return result    \n",
    "def initiate_predictions(train_test_data,method,multiclass):\n",
    "    X_train = train_test_data[0]\n",
    "    X_test = train_test_data[1]\n",
    "    y_train = train_test_data[2]\n",
    "    y_test = train_test_data[3]\n",
    "    prediction = train_predict_model(method,X_train,X_test,y_train,y_test,multiclass)\n",
    "    predicted = prediction[0]\n",
    "    acc = prediction[1]\n",
    "    true = y_test\n",
    "    \n",
    "    pred_prob = prediction[2]\n",
    "    if method == None:\n",
    "        loss = prediction[2]\n",
    "        pred_prob = prediction[3]\n",
    "        result = [true,predicted,acc,loss,pred_prob]\n",
    "    else:\n",
    "        result = [true,predicted,acc,pred_prob]\n",
    "    return result\n",
    "def plot_cm(cm1,cm2,method):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.imshow(cm1, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm1[i][j]))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    plt.imshow(cm2, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm2[i][j]))\n",
    "    plt.plot()\n",
    "    plt.savefig('confusionmatrix'+method+'.jpg')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def result_from_predicitions(prediction_array):\n",
    "    \n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*30)\n",
    "    df1=pd.DataFrame({'Actual':prediction_array[0], 'Predicted':prediction_array[1]})\n",
    "    print(df1)\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(prediction_array[0],prediction_array[1]),4))\n",
    "    print('Precision:', np.round(metrics.precision_score(prediction_array[0],prediction_array[1],average='weighted'),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(prediction_array[0],prediction_array[1],average='weighted'),4))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(prediction_array[0],prediction_array[1],average='weighted'),4))\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    print(metrics.classification_report(prediction_array[0],prediction_array[1]))\n",
    "    \n",
    "#AUCROC for binary class only\n",
    "def plot_roc(result1,result2,title,method):\n",
    "    #courtesy of DATAI https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv\n",
    "    # plot arrows, why? to present accuracy\n",
    "    fig1 = plt.figure(figsize=[20,10])\n",
    "    ax1 = fig1.add_subplot(121,aspect = 'equal')\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    #for i in range(len(result1[3])):\n",
    "    fpr, tpr, _ = roc_curve(result1[0], result1[3])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label=' (AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='red',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC '+title[0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "\n",
    "    ax2 = fig1.add_subplot(122,aspect = 'equal')\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    #for i in range(len(result2[3])):\n",
    "    fpr, tpr, _ = roc_curve(result2[0], result2[3])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label='(AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC ' + title[1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "    \n",
    "    plt.savefig('roc'+method+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#perform predictions on classification methods\n",
    "def clf_predictor(input_data,multiclass):\n",
    "    result_logregr = predictor(input_data,None,multiclass)\n",
    "    result_NBG = predictor(input_data,'NBGauss',multiclass)\n",
    "    result_NBM = predictor(input_data,'NBMulti',multiclass)\n",
    "    result_SVM = predictor(input_data,'SVM',multiclass)\n",
    "    result_RF = predictor(input_data,'RF',multiclass)\n",
    "    result_ensemble = predictor(input_data,'ensemble',multiclass)\n",
    "    result_GB = predictor(input_data,'gb',multiclass)\n",
    "    print(\"Evaluation of predictions:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_logregr)\n",
    "    #plots the graph for accuracies over different encoding metods for one particular classification method\n",
    "    print(\"Logistic Regression:\")\n",
    "    print(\"----------------------------------\")\n",
    "    result_logres_acc = [result_logregr[0][2],result_logregr[1][2]]\n",
    "    result_logres_loss = [result_logregr[0][3],result_logregr[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    if multiclass == 'no':\n",
    "        print(\"Naive Bayes (Gaussian):\")\n",
    "        generate_metrics(result_NBG,'NBG')\n",
    "        result_NBG_acc = [result_NBG[0][2],result_NBG[1][2]]\n",
    "        result_NBG_loss = [result_NBG[0][3],result_NBG[1][3]]\n",
    "        print(\"----------------------------------\")\n",
    "    elif multiclass == 'yes':\n",
    "        print(\"Naive Bayes (Multinomial):\")\n",
    "        print(\"----------------------------------\")\n",
    "        generate_metrics(result_NBM,'NBM')\n",
    "        result_NBM_acc = [result_NBM[0][2],result_NBM[1][2]]\n",
    "        result_NBM_loss = [result_NBM[0][3],result_NBM[1][3]]\n",
    "        print(\"----------------------------------\")\n",
    "    print(\"Support Vector Machine:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_SVM,'SVM')\n",
    "    result_SVM_acc = [result_SVM[0][2],result_SVM[1][2]]\n",
    "    result_SVM_loss = [result_SVM[0][3],result_SVM[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Gradient Boosting\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_GB,'gb')\n",
    "    result_GB_acc = [result_GB[0][2],result_GB[1][2]]\n",
    "    result_GB_loss = [result_GB[0][3],result_GB[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Random Forest:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_RF,'RF')\n",
    "    result_RF_acc = [result_RF[0][2],result_RF[1][2]]\n",
    "    result_RF_loss = [result_RF[0][3],result_RF[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Ensemble of Logistic Regression, Naive Bayes (Multinomial) and Random Forest:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_ensemble,'ensemble')\n",
    "    result_ensemble_acc = [result_ensemble[0][2],result_ensemble[1][2]]\n",
    "    result_ensemble_loss = [result_ensemble[0][3],result_ensemble[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    result_acc = [result_logres_acc[0],result_logres_acc[1], \\\n",
    "    result_NBG_acc[0],result_NBG_acc[1], \\\n",
    "    result_NBM_acc[0],result_NBM_acc[1], \\\n",
    "    result_SVM_acc[0],result_SVM_acc[1], \\\n",
    "    result_RF_acc[0],result_RF_acc[1], \\\n",
    "    result_ensemble_acc[0],result_ensemble_acc[1],result_GB_acc[0],result_GB_acc[1]]\n",
    "\n",
    "    result_loss = [result_logres_loss[0],result_logres_loss[1], \\\n",
    "    result_NBG_loss[0],result_NBG_loss[1], \\\n",
    "    result_NBM_loss[0],result_NBM_loss[1], \\\n",
    "    result_SVM_loss[0],result_SVM_loss[1], \\\n",
    "    result_RF_loss[0],result_RF_loss[1], \\\n",
    "    result_ensemble_loss[0],result_ensemble_loss[1],result_GB_loss[0],result_GB_loss[1]]\n",
    "\n",
    "    proba_data = [result_logregr[0][4],result_logregr[1][4],result_NBG[0][4],result_NBG[1][4],\\\n",
    "        result_NBM[0][4],result_NBM[1][4],result_SVM[0][4],result_SVM[1][4],\\\n",
    "        result_GB[0][4],result_GB[1][4],result_RF[0][4],result_RF[1][4],\\\n",
    "        result_ensemble[0][4],result_ensemble[1][4]]\n",
    "    test_data = [result_logregr[0][1],result_logregr[1][1],result_NBG[0][1],result_NBG[1][1],\\\n",
    "        result_NBM[0][1],result_NBM[1][1],result_SVM[0][1],result_SVM[1][1],\\\n",
    "        result_GB[0][1],result_GB[1][1],result_RF[0][1],result_RF[1][1],\\\n",
    "        result_ensemble[0][1],result_ensemble[1][1]]\n",
    "  \n",
    "    result1 = [result_acc, result_loss]\n",
    "    result2 = [proba_data,test_data]\n",
    "    result=[result1,result2]\n",
    "    return result\n",
    "#perform word embeddings inputs: dataframe input data and output data; output: embedded data such as X train and test and y train and test\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('swedish')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        \n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "#category is binary by default\n",
    "def word_embeddings(input_data, output_data,ANN,dense,el,category=None):\n",
    "    \n",
    "    #Don't use stop words removal for deep learning\n",
    "    #input_data = input_data.apply(remove_stopwords)\n",
    "    \n",
    "    data = feature_engineering(input_data, output_data)\n",
    "    '''\n",
    "    global model2\n",
    "    resmod2 = [None] * input_data.shape[0]\n",
    "    #print(np.array(input_data)[0])\n",
    "    for i in range(input_data.shape[0]):\n",
    "        resmod = model2.predict(np.array(input_data)[i])\n",
    "        print(resmod[0])\n",
    "        lst = list(resmod)\n",
    "        if lst[0] == \"('label_1',)\":\n",
    "            lst[0] = 1\n",
    "        else:\n",
    "            lst[0] = 0\n",
    "        \n",
    "        resmod2.append(tuple(lst))\n",
    "    print(resmod2)\n",
    "    resmod = [value for x in resmod2 for value in x and value is not None]\n",
    "    resmod = np.array(resmod2)\n",
    "    '''\n",
    "\n",
    "\n",
    "    print(\"Train set has total {0} entries with {1:.2f}% 0, {2:.2f}% 1\".format(len(data[0]),\n",
    "                                                                             (len(data[0][data[2] == 0]) / (len(data[0])*1.))*100,\n",
    "                                                                        (len(data[0][data[2] == 1]) / (len(data[0])*1.))*100))\n",
    "    print(\"Test set has total {0} entries with {1:.2f}% 0, {2:.2f}% 1\".format(len(data[1]),\n",
    "                                                                             (len(data[1][data[3] == 0]) / (len(data[1])*1.))*100,\n",
    "                                                                            (len(data[1][data[3] == 1]) / (len(data[1])*1.))*100))\n",
    "    \n",
    "    data_out = we_output_data_transform(data[2],data[3])\n",
    "    '''\n",
    "    if category == None:\n",
    "        data = feature_engineering(input_data, output_data)\n",
    "        data_out = we_output_data_transform(data[2],data[3])\n",
    "    else:\n",
    "        with open('df.pickle', 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "        count = len(df.index)\n",
    "        data = feature_engineering(input_data, output_data)\n",
    "        dataset,data_out = we_output_data_transform(data[2],data[3],'multi')\n",
    "        data_in1 = multivectorizer(dataset)\n",
    "        data_in2 = feature_engineering(data_in1[0], data_out[:count])\n",
    "        #validation data\n",
    "        data2 = feature_engineering(data_in2[0], data_out[0])\n",
    "    '''\n",
    "    \n",
    "    #index 0 = X_train\n",
    "    #index 1 = X_test\n",
    "    #index 2 = y_train\n",
    "    #index 3 = y_test\n",
    "    assert data[0].shape[0] == data[2].shape[0]\n",
    "    assert data[1].shape[0] == data[3].shape[0]\n",
    "    data_in1 = tokenizer(input_data,data[0], data[1])\n",
    "    \n",
    "    print(data_in1[2])\n",
    "    data_in2 = padding(data_in1[0], data_in1[1],input_data)\n",
    "    global MAX_SEQUENCE_LENGTH\n",
    "    MAX_SEQUENCE_LENGTH = data_in2[0].shape[1]\n",
    "    '''\n",
    "    data_in21 = vectorize_sequences(input_data)\n",
    "    '''\n",
    "    \n",
    "    #create validation data \n",
    "    data2 = feature_engineering(data_in2[0], data_out[0])\n",
    "    #data2[0] = X_val\n",
    "    #data2[2] = y_val\n",
    "    assert data2[1].shape[0] == data2[3].shape[0]\n",
    "    assert data2[0].shape[0] == data2[2].shape[0]\n",
    "    #fasttext (word_to_vec_map, word_to_index, index_to_words, vocab_size, dim)\n",
    "    #tip: try to swap the sv.vec file with cc.sv.300.vec\n",
    "    #load fasttext data into cnn1\n",
    "    #save\n",
    "    with open('data_in2.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_in2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data_out.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_out, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    '''\n",
    "    with open('data_in.pickle', 'rb') as handle:\n",
    "        data_in = pickle.load(handle)\n",
    "    with open('data_out.pickle', 'rb') as handle:\n",
    "        data_out = pickle.load(handle)\n",
    "    '''\n",
    "   \n",
    "    corpus = load_vectors2('./data/fasttext/sv.vec')\n",
    "    #data_train = sentences_to_indices(data[0],corpus[1],len(data_in1[2]))\n",
    "    #data_test = sentences_to_indices(data[1],corpus[1],len(data_in1[2]))\n",
    "    '''\n",
    "    wiki_news = './cc.sv.300.vec'\n",
    "    embed_fasttext = load_embed(wiki_news)\n",
    "    #step1 build vocab\n",
    "    vocab = build_vocab(input_data)\n",
    "    #vocab is your embedding matrix\n",
    "    #step2 check coverage\n",
    "    print(\"FastText : \")\n",
    "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "    print(oov_fasttext[:18])\n",
    "    '''\n",
    "    vocab = None\n",
    "    embedding_layer1 = pretrained_embedding_layer(corpus[0], corpus[1],vocab)\n",
    "    print(corpus[4])\n",
    "    \n",
    "    embedding_layer0=load_vectors_word2vec('./data/word2vec/sv.bin',data_in1[2])\n",
    "    \n",
    "    embedding_layer = [embedding_layer0,embedding_layer1]\n",
    "    #second preprocessing method\n",
    "    #change data_in2[0].shape[1] with (MAX_LEN,)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(date_time(1))\n",
    "    model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_in2[0],data_out[0],data2[0],data2[2],dense)\n",
    "    #or\n",
    "    #model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_train,data_out[0],None,None,dense)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print(\"\\nElapsed Time: \" + elapsed_time)\n",
    "    print(\"Completed Model Trainning\", date_time(1))\n",
    "    '''\n",
    "    with open('model'+ANN+'.pickle', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    '''\n",
    "    #data_in2[0] = X_train\n",
    "    #data[2] = y_train\n",
    "    return [data_in2, data2, data,model]\n",
    "def vectorize_sequences(sequences, dimension=4900):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "def multivectorization(concated):\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(concated['freetext'].values)\n",
    "    sequences = tokenizer.texts_to_sequences(concated['freetext'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "    result = [X,word_index]\n",
    "    return result\n",
    "def we_evaluation(model,model1,data1,data2,data3,data4,ANN1,ANN2 ,datax,datay):\n",
    "    \n",
    "    preds1 = model.predict(data1[1],batch_size=13)\n",
    "    preds2 = model1.predict(data3[1],batch_size=13)\n",
    "    preds1 = np.argmax(preds1, axis=-1)\n",
    "    preds2 = np.argmax(preds2, axis=-1)\n",
    "    with open('preds1.pickle', 'wb') as handle:\n",
    "        pickle.dump(preds1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('preds2.pickle', 'wb') as handle:\n",
    "        pickle.dump(preds2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('test_data1.pickle', 'wb') as handle:\n",
    "        pickle.dump(datax[3], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('test_data2.pickle', 'wb') as handle:\n",
    "        pickle.dump(datay[3], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #or\n",
    "    #preds2 = model.predict_classes(data_in2[1])\n",
    "    '''\n",
    "    Call the metrics function\n",
    "    '''\n",
    "\n",
    "  \n",
    "    test1 = ANN1\n",
    "    test2 = ANN2\n",
    "    title = [test1,test2]\n",
    "    #keras evaluation\n",
    "    score = model.evaluate(data2[1], data2[3], verbose=0)\n",
    "    print(\"Model Performance of model 1: \"+ test1 +\" (Test):\")\n",
    "    df_score1=pd.DataFrame.from_records([{'Accuracy':score[1],'Precision':score[2],'Recall':score[3],'F1_score':score[4]}])\n",
    "    print(df_score1)\n",
    "    score = model1.evaluate(data4[1], data4[3], verbose=0)\n",
    "    print(\"Model Performance of model 2: \"+ test2 +\" (Test):\")\n",
    "    df_score2=pd.DataFrame.from_records([{'Accuracy':score[1],'Precision':score[2],'Recall':score[3],'F1_score':score[4]}])\n",
    "    print(df_score2)\n",
    "    '''\n",
    "    result1 = pd.DataFrame({'model': test1, 'score': accuracy1[1]*100}, index=[-1])\n",
    "    result2 = pd.DataFrame({'model': test2, 'score': accuracy2[1]*100}, index=[-1])\n",
    "    result = pd.concat([result2, result1.ix[:]]).reset_index(drop=True)\n",
    "    plot_model_performace(result)\n",
    "    '''\n",
    "    #load pickle\n",
    "    with open('preds1.pickle', 'rb') as handle:\n",
    "        preds1 = pickle.load(handle)\n",
    "    \n",
    "    with open('preds2.pickle', 'rb') as handle:\n",
    "        preds2 = pickle.load(handle)\n",
    "    \n",
    "    with open('test_data1.pickle', 'rb') as handle:\n",
    "        test_data1 = pickle.load(handle)\n",
    "    \n",
    "    with open('test_data2.pickle', 'rb') as handle:\n",
    "        test_data2 = pickle.load(handle)\n",
    "   \n",
    "    '''\n",
    "    test_data1 = pd.Series(test_data1)\n",
    "    test_data2 = pd.Series(test_data2)\n",
    "    '''\n",
    "    \n",
    "    #SKLearn Evaluation\n",
    "    target_class = ['class_0','class_1']\n",
    "    labels = [0,1]\n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*200)\n",
    "    df1=pd.DataFrame({'Actual':test_data1, 'Predicted':preds1})\n",
    "    print(df1)\n",
    "    print('-'*200)\n",
    "    df2=pd.DataFrame({'Actual':test_data2, 'Predicted':preds2})\n",
    "    print(df2)\n",
    "    print('-'*200)\n",
    "    print(metrics.classification_report(test_data1,preds1,labels,target_class))\n",
    "    print('-'*200)\n",
    "    print(metrics.classification_report(test_data2,preds2,labels,target_class))\n",
    "    array1 = [test_data1,preds1]\n",
    "    array2 = [test_data2,preds2]\n",
    "    method = test1+'_'+test2\n",
    "    plot_classification_report(array1,array2,title,method)\n",
    "    print('-'*200)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*200)\n",
    "    cm1 = metrics.confusion_matrix(y_true=test_data1, y_pred=preds1)\n",
    "    print(cm1)\n",
    "    cm2 = metrics.confusion_matrix(y_true=test_data2, y_pred=preds2)\n",
    "    print(cm2)\n",
    "    plot_cm(cm1,cm2,method)\n",
    "    df1.to_csv('prediction1.csv', encoding='utf-8', index=True)\n",
    "    df2.to_csv('prediction2.csv', encoding='utf-8', index=True)\n",
    "def sentences_to_indices(X, word_to_index, maxLen):\n",
    "    m = X.shape[0] \n",
    "    X = np.array(X)\n",
    "    X_indices = np.zeros((m, maxLen))\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().strip().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index:\n",
    "                w = \"person\"  \n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices\n",
    "def plot_function(track):\n",
    "    plt.subplot(221)\n",
    "    plt.plot(track.history['acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(track.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.show()\n",
    "def plot_classification_report(array1, array2,title,method, ax=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(211)\n",
    "    \n",
    "    plt.title('Classification Report '+title[0])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array1[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array1[0], array1[1])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    \n",
    "    plt.title('Classification Report '+title[1])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array2[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array2[0], array2[1])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "    \n",
    "    \n",
    "    plt.savefig('classificationreport'+ method +'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#tokenizes the words\n",
    "def tokenizer2(train_data, test_data):\n",
    "    train = fastText.tokenize(train_data)\n",
    "    test = fastText.tokenize(test_data)\n",
    "    result = [train,test]\n",
    "    return result\n",
    "def tokenizer(input_data,train_data, test_data):\n",
    "    #from [7]\n",
    "    seq_lengths = input_data.apply(lambda x: len(x.split(' ')))\n",
    "    print(seq_lengths.describe())\n",
    "    \n",
    "    \n",
    "    max_sequence_len = max([len(x) for x in input_data])\n",
    "    print(max_sequence_len)\n",
    "    \n",
    "    tk = Tokenizer(num_words=NB_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \")\n",
    "    tk.fit_on_texts(input_data)\n",
    "    trained_seq = tk.texts_to_sequences(train_data)\n",
    "    test_seq = tk.texts_to_sequences(test_data)\n",
    "    word_index = tk.word_index\n",
    "    result = [trained_seq, np.array(test_seq), word_index]\n",
    "    return result\n",
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n",
    "#test function from [7] to make sure that the sequences generated from the tokenizer function are of equal length\n",
    "def test_sequence(train_data):\n",
    "    seq_lengths = train_data.apply(lambda x: len(x.split(' ')))\n",
    "    print(\"The sequences generated are:\")\n",
    "    seq_lengths.describe()\n",
    "    print(\"----------------\")\n",
    "def date_time(x):\n",
    "    if x==1:\n",
    "        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==2:    \n",
    "        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==3:  \n",
    "        return 'Date now: %s' % datetime.datetime.now()\n",
    "    if x==4:  \n",
    "        return 'Date today: %s' % datetime.date.today() \n",
    "def plot_performance(history=None, figure_directory=None, ylim_pad=[0, 0]):\n",
    "    xlabel = 'Epoch'\n",
    "    legends = ['Training', 'Validation']\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    y1 = history.history['f1_score']\n",
    "    y2 = history.history['val_f1_score']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[0]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[0]\n",
    "\n",
    "    plt.subplot(221)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model F1_score\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('F1_score', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['loss']\n",
    "    y2 = history.history['val_loss']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(222)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Loss', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['precision']\n",
    "    y2 = history.history['val_precision']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(223)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Precision\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Precision', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['recall']\n",
    "    y2 = history.history['val_recall']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(224)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Recall\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Recall', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "    if figure_directory:\n",
    "        plt.savefig(figure_directory+\"/history\")\n",
    "\n",
    "    plt.show()\n",
    "#in [7] padding is used to fill out null values\n",
    "def padding(trained_seq, test_seq,input):\n",
    "    \n",
    "    \n",
    "    trained_seq_trunc = pad_sequences(trained_seq,maxlen=MAX_LEN)\n",
    "    \n",
    "    test_seq_trunc = pad_sequences(test_seq,maxlen=MAX_LEN)\n",
    "    print(trained_seq_trunc.shape)\n",
    "    print(test_seq_trunc.shape)\n",
    "    result = [trained_seq_trunc, test_seq_trunc]\n",
    "    return result\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == './cc.sv.300.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "def we_output_data_transform(y_train,y_test,encoding='binary',category=None):\n",
    "    if encoding == 'multi':\n",
    "        '''\n",
    "        From Peter Nagy, Kaggle, https://www.kaggle.com/ngyptr/multi-class-classification-with-lstm\n",
    "        '''\n",
    "        with open('df.pickle', 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "        df1 = df['freetext']\n",
    "        df2 = df['pout']\n",
    "        frames = [df1,df2]\n",
    "        data = pd.concat(frames, axis=1, sort=False)\n",
    "        num_of_categories = df.count+1\n",
    "        shuffled = data.reindex(np.random.permutation(data.index))\n",
    "        prio1A = shuffled[shuffled['pout'] == '1A'][:num_of_categories]\n",
    "        prio1B = shuffled[shuffled['pout'] == '1B'][:num_of_categories]\n",
    "        prio2A = shuffled[shuffled['pout'] == '2A'][:num_of_categories]\n",
    "        prio2B = shuffled[shuffled['pout'] == '2B'][:num_of_categories]\n",
    "        Referal = shuffled[shuffled['pout'] == 'Referal'][:num_of_categories]\n",
    "        concated = pd.concat([prio1A,prio1B,prio2A,prio2B,Referal], ignore_index=True)\n",
    "        concated = concated.reindex(np.random.permutation(concated.index))\n",
    "        concated['LABEL'] = 0\n",
    "        concated.loc[concated['pout'] == '1A', 'LABEL'] = 0\n",
    "        concated.loc[concated['pout'] == '1B', 'LABEL'] = 1\n",
    "        concated.loc[concated['pout'] == '2A', 'LABEL'] = 2\n",
    "        concated.loc[concated['pout'] == '2B', 'LABEL'] = 3\n",
    "        concated.loc[concated['pout'] == 'Referal', 'LABEL'] = 4\n",
    "        labels = to_categorical(concated['LABEL'], num_classes=5)\n",
    "        if 'pout' in concated.keys():\n",
    "            concated.drop(['pout'], axis=1)\n",
    "        return concated,labels\n",
    "    else:\n",
    "        le = LabelEncoder()\n",
    "        #y_train_le = le.fit_transform(y_train.values)\n",
    "        #y_test_le = le.fit_transform(y_test.values)\n",
    "        y_train = to_categorical(y_train.values).astype('float32')\n",
    "        y_test = to_categorical(y_test.values).astype('float32')\n",
    "    result = [y_train,y_test]\n",
    "    return result\n",
    "#embeddings layer\n",
    "def embeddings_layer(X_train_emb, X_valid_emb,y_train_emb,y_valid_emb,dense):\n",
    "    emb_model = models.Sequential()\n",
    "    emb_model.add(layers.Embedding(NB_WORDS, DIM, input_length=MAX_LEN))\n",
    "    emb_model.add(layers.Flatten())\n",
    "    emb_model.add(layers.Dense(dense, activation='softmax'))\n",
    "    emb_model.summary()\n",
    "    emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)\n",
    "    result = emb_history\n",
    "    return result\n",
    "#from [7]\n",
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=1)\n",
    "    \n",
    "    return history\n",
    "#load vec file from fasttext, from [8]\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    vocab_size, dim = map(int, fin.readline().split())\n",
    "    word_to_vec_map = {}\n",
    "    words = set()\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        words.add(tokens[0])\n",
    "        word_to_vec_map[tokens[0]] = np.array(tokens[1:], dtype=np.float64)\n",
    "    i = 1\n",
    "    words_to_index = {}\n",
    "    index_to_words = {}\n",
    "    for w in sorted(words):\n",
    "        words_to_index[w] = i\n",
    "        index_to_words[i] = w\n",
    "        i = i + 1\n",
    "    return word_to_vec_map, words_to_index, index_to_words, vocab_size, dim\n",
    "#caller function for load_vectors .vec file\n",
    "def load_vectors2(fname):\n",
    "    word_to_vec_map, words_to_index, index_to_words, vocab_size, dim = load_vectors(fname)\n",
    "    result = [word_to_vec_map, words_to_index, index_to_words, vocab_size, dim]\n",
    "    return result\n",
    "#from[8]\n",
    "#for Word2Vec input word2vec .bin file and word_index = tokenizer.word_index from tokenizer\n",
    "def load_vectors_word2vec(fname,word_index):\n",
    "    word_vectors = KeyedVectors.load(fname)\n",
    "    \n",
    "    vocabulary_size = min(MAX_NB_WORDS, len(word_index))+1\n",
    "    embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i>=NB_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "    del(word_vectors)\n",
    "    embedding_layer = Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "    embedding_layer.build(MAX_LEN)\n",
    "    return embedding_layer\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")\n",
    "#[8]\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index,embeddings_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    #emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    emb_dim = 300\n",
    "    '''\n",
    "    #or\n",
    "    emb_dim = 300\n",
    "    '''\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        '''\n",
    "        if index >= NB_WORDS: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: emb_matrix[index] = embedding_vector\n",
    "        '''\n",
    "    embedding_layer = Embedding(input_dim = vocab_len, output_dim = emb_dim,\n",
    "    mask_zero = False,input_length = MAX_SEQUENCE_LENGTH,trainable=False) \n",
    "\n",
    "    embedding_layer.build(MAX_LEN)\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "#bilstm\n",
    "def bilstm(input_shape,embedding_layer1,dense):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Bidirectional(LSTM(300)))\n",
    "    model.add(Dense(600,activation='elu'))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.009,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def cnn2(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='elu', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(300,return_sequences=True))\n",
    "    model.add(GRU(300))\n",
    "    model.add(Dense(512,activation='elu'))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def cnn1(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(300))\n",
    "    model.add(Dense(512,activation='elu'))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def gru_model(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(GRU(117))\n",
    "    #model.add(Dense(234))\n",
    "    #model.add(Dense(3))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "def lstm_model(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(LSTM(117))\n",
    "    #model.add(Dense(234,activation='elu',kernel_regularizer=regularizers.l2(1e-9),activity_regularizer=regularizers.l1(1e-9),bias_regularizer=regularizers.l2(0.01), kernel_constraint=maxnorm(3)))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "    #from https://www.kaggle.com/sakarilukkarinen/embedding-lstm-gru-and-conv1d/versions\n",
    "def gru_model2(input_shape,embedding_layer1,dense):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(GRU(lstm_out,kernel_initializer='random_uniform',dropout=0.00001,recurrent_dropout=0.00001,activation='elu',use_bias=True,return_sequences=True))\n",
    "    model.add(GRU(MAX_LEN,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(lstm_out,activation='elu'))\n",
    "    \n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense, use_bias=False,activation='softmax',kernel_regularizer=regularizers.l2(1e-16),activity_regularizer=regularizers.l1(1e-16),bias_regularizer=regularizers.l2(0.01), kernel_constraint=maxnorm(3)))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.009,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def predict_model(input_shape,embedding_layer,model_type,X_train,y_train,X_val,y_val,dense):\n",
    "    callbacks = [EarlyStopping(monitor='val_loss',patience=2)]\n",
    "    #you can also use rmsprop as optimizer\n",
    "    #adam = Adam(lr=1e-3)\n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    #loss = 'categorical_crossentropy'\n",
    "    if model_type == 'cnn1':\n",
    "        model = cnn1((input_shape,),embedding_layer,dense)\n",
    "        #model.compile(loss=loss,optimizer=adam,metrics=['acc'])\n",
    "        #for train, test in kfold.split(X, Y):\n",
    "        track = model.fit(X_train, y_train, batch_size=13, epochs=20, verbose=1,validation_data=(X_val, y_val))\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        [9] https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7\n",
    "        By Ricky Kim, Another Twitter sentiment analysis with Python — Part 9 (Neural Networks with Tfidf vectors using Keras)\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'cnn2':\n",
    "        model = cnn2((input_shape,),embedding_layer,dense)\n",
    "        #model1.compile(loss=loss,optimizer=adam,metrics=['acc'])\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, batch_size=13, epochs=20, verbose=1,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track2)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'lstm':\n",
    "        model = lstm_model((input_shape,),embedding_layer,dense)\n",
    "        #The model has already been compiled in the function call\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'bilstm':\n",
    "        model = bilstm((input_shape,),embedding_layer,dense)\n",
    "        #The model has already been compiled in the function call\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'gru':\n",
    "        model = gru_model((input_shape,),embedding_layer,dense)\n",
    "        #The model has already been compiled in the function call\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "        \n",
    "    else:\n",
    "        model = gru_model2((input_shape,),embedding_layer,dense)\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        \n",
    "        plot_performance(track)\n",
    "    #print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "    #model = None\n",
    "    return model\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "def plot_model_performace(result):\n",
    "    \n",
    "    sns.set_style(\"ticks\")\n",
    "    figsize=(22, 6)\n",
    "\n",
    "    ticksize = 12\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    xlabel = \"Model\"\n",
    "    ylabel = \"Score\"\n",
    "\n",
    "    title = \"Model Performance\"\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "              'axes.labelsize' : labelsize,\n",
    "              'axes.titlesize' : titlesize,\n",
    "              'xtick.labelsize': ticksize,\n",
    "              'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "\n",
    "    col1 = \"model\"\n",
    "    col2 = \"score\"\n",
    "    sns.barplot(x=col1, y=col2, data=result)\n",
    "\n",
    "    plt.title(title.title())\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid()\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split), ignore_index=True)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "#to call the function above, do this: parallelize_dataframe(df, cleaning), then pickle\n",
    "#from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras, visited 26th may\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "'''\n",
    "[1] https://medium.com/deep-learning-turkey/text-processing-1-old-fashioned-methods-bag-of-words-and-tfxidf-b2340cc7ad4b, Medium, Deniz Kilinc visited 6th of April 2019\n",
    "[2] https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm, from ReiiNakano , Kaggle, visited 5th of April 2019\n",
    "[3] https://github.com/codebasics/py/tree/master/ML, github, Codebasics from dhavalsays, visited 6th of April 2019\n",
    "[4] from scikit-learn.org (base code), visited 4th of April 2019\n",
    "[5] Python One Hot Encoding with SciKit Learn, InsightBot, http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn, visited 6th April 2019 \n",
    "[6] Kaggle, Sentiment Analysis : CountVectorizer & TF-IDF, Divyojyoti Sinha, https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\n",
    "[7] Kaggle, Bert Carremans, Using Word Embeddings for Sentiment Analysis, https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis, visited april 11th 2019\n",
    "[8] Sentiment Analysis with pretrained Word2Vec, Varun Sharma, Kaggle, https://www.kaggle.com/varunsharmaml/sentiment-analysis-with-pretrained-word2vec, visited 12th of april 2019\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame(columns=[\"Method\",\"Accuracy\"])\n",
    "df = pd.read_csv(\"testdata_exjobb.csv\", encoding='ISO-8859-1')\n",
    "df = transformAge(df)\n",
    "df = transformLCD(df)\n",
    "input_data = df.FreeText.astype(str)\n",
    "input_data1 = df.iloc[:,1:2].FreeText.astype(str)\n",
    "output_data = df.hosp_ed\n",
    "preproc1 = pre_processing1(input_data,df)\n",
    "bow = text_processing(input_data,output_data, None, 1)\n",
    "tfidf = text_processing(input_data,output_data, \"tfidf\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from Bag of Words on Naive_Bayes_Gaussian:\n",
      "------------------------------\n",
      "Results from prediction:\n",
      "------------------------------\n",
      "    Actual  Predicted\n",
      "1        1          1\n",
      "9        0          1\n",
      "2        0          1\n",
      "4        1          1\n",
      "14       1          1\n",
      "8        1          0\n",
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.5\n",
      "Precision: 0.4\n",
      "Recall: 0.5\n",
      "F1 Score: 0.4444\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         6\n",
      "   macro avg       0.30      0.38      0.33         6\n",
      "weighted avg       0.40      0.50      0.44         6\n",
      "\n",
      "------------------------------\n",
      "Metrics from TF-IDF on Naive_Bayes_Gaussian:\n",
      "------------------------------\n",
      "Results from prediction:\n",
      "------------------------------\n",
      "    Actual  Predicted\n",
      "1        1          1\n",
      "9        0          1\n",
      "2        0          0\n",
      "4        1          0\n",
      "14       1          1\n",
      "8        1          0\n",
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5556\n",
      "Recall: 0.5\n",
      "F1 Score: 0.5143\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.56      0.50      0.51         6\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Confusion Matrix:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[[0 2]\n",
      " [1 3]]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[[1 1]\n",
      " [2 2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "method = 'NBG'\n",
    "data_array = [bow,tfidf]\n",
    "multiclass = 'no'\n",
    "result = predictor(data_array,method,multiclass)\n",
    "generate_metrics(result,method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
