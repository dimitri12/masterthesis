{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[1] https://medium.com/deep-learning-turkey/text-processing-1-old-fashioned-methods-bag-of-words-and-tfxidf-b2340cc7ad4b, Medium, Deniz Kilinc visited 6th of April 2019\\n[2] https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm, from ReiiNakano , Kaggle, visited 5th of April 2019\\n[3] https://github.com/codebasics/py/tree/master/ML, github, Codebasics from dhavalsays, visited 6th of April 2019\\n[4] from scikit-learn.org (base code), visited 4th of April 2019\\n[5] Python One Hot Encoding with SciKit Learn, InsightBot, http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn, visited 6th April 2019 \\n[6] Kaggle, Sentiment Analysis : CountVectorizer & TF-IDF, Divyojyoti Sinha, https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\\n[7] Kaggle, Bert Carremans, Using Word Embeddings for Sentiment Analysis, https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis, visited april 11th 2019\\n[8] Sentiment Analysis with pretrained Word2Vec, Varun Sharma, Kaggle, https://www.kaggle.com/varunsharmaml/sentiment-analysis-with-pretrained-word2vec, visited 12th of april 2019\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from IPython import get_ipython\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    ipy.run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import re\n",
    "import io\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim\n",
    "import keras.backend as K\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scikitplot.plotters as skplt\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Activation, LeakyReLU, PReLU, ELU, ThresholdedReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import unit_norm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from gensim.models import FastText\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Conv1D, MaxPooling1D, Dropout, Concatenate, Conv2D, MaxPooling2D, concatenate,BatchNormalization, Bidirectional\n",
    "from keras.initializers import glorot_uniform\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.activations import relu\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import seaborn as sns\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from gensim.models.wrappers import FastText\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import datetime\n",
    "import fastText\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import string\n",
    "import operator \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "num_partitions = multiprocessing.cpu_count()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "#Global Variables from [7] and others\n",
    "NB_WORDS = 9501  # Parameter indicating the number of words we'll put in the dictionary \n",
    "\n",
    "VAL_SIZE = 9  # Size of the validation set (originally 1000)\n",
    "NB_START_EPOCHS = 8  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 72  # Maximum number of words in a sequence\n",
    "#MAX_LEN = 62\n",
    "GLOVE_DIM = 50  # Number of dimensions of the GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 456\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "#activation function\n",
    "act = 'relu'\n",
    "re_weight = True\n",
    "#dimension for fasttext\n",
    "DIM = 300\n",
    "#dimension for word2vec\n",
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "dim = 50\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "maxLen = 60\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "lstm_output_size = 70\n",
    "#model2 = fastText.train_supervised('./fasttext_train.txt',label='label_', epoch=20, dim=200)\n",
    "\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\s\\W',' ',s)\n",
    "    s = re.sub(r'\\W,\\s',' ',s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(r\"\\d+\", \"\", s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\",\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    \n",
    "    return s\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode('utf8', 'ignore')\n",
    "    return txt\n",
    "\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    data['FreeText'] = [cleaning(s) for s in data['FreeText']]\n",
    "    corpus = []\n",
    "    for col in ['FreeText']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "    \n",
    "    with open('corpus.pickle', 'wb') as handle:\n",
    "        pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return corpus\n",
    "def tsne_plot(df):\n",
    "    corpus = build_corpus(df)\n",
    "    \n",
    "    model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=1, workers=4)\n",
    "    with open('w2vmodel.pickle', 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #model.most_similar('')\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "def transformAge(df):\n",
    "    '''\n",
    "    This function will categorise the age data into 5 different age categories\n",
    "    '''\n",
    "    dfage = df.Age\n",
    "\n",
    "    array = []\n",
    "    for i in range(len(dfage)):\n",
    "        if dfage[i] < 3:\n",
    "            array.append(0)\n",
    "        elif dfage[i] >= 3 and dfage[i] < 13:\n",
    "            array.append(1)\n",
    "        elif dfage[i] >= 13 and dfage[i] < 19:\n",
    "            array.append(2)\n",
    "        elif dfage[i] >= 19 and dfage[i] < 65:\n",
    "            array.append(3)\n",
    "        else:\n",
    "            array.append(4)\n",
    "    df[\"AgeCat\"] = array\n",
    "    return df\n",
    "\n",
    "def transformLCD(df):\n",
    "    '''\n",
    "    This function will transform the numerical LCD data into 3 categories\n",
    "    '''\n",
    "    dflcd = df.LastContactDays\n",
    "    array = []\n",
    "    for i in range(len(dflcd)):\n",
    "        if dflcd[i] < 2:\n",
    "            array.append(0)\n",
    "        elif dflcd[i] >= 2 and dflcd[i] < 31:\n",
    "            array.append(1)\n",
    "        else:\n",
    "            array.append(2)\n",
    "    df[\"LcdCat\"] = array\n",
    "    return df\n",
    "\n",
    "def test_code(input):\n",
    "    print(\"Hello \" + input)\n",
    "\n",
    "def eda1(df):\n",
    "    figsize=(20, 10)\n",
    "    \n",
    "    ticksize = 14\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "            'axes.labelsize' : labelsize,\n",
    "            'axes.titlesize' : titlesize,\n",
    "            'xtick.labelsize': ticksize,\n",
    "            'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "    \n",
    "    plt.subplot(441)\n",
    "    x1=df['prio']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x1)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Priority')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "\n",
    "    plt.subplot(442)\n",
    "    x2=df['operator']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x2)\n",
    "   # plt.title(\"Review Sentiment Count\")\n",
    "    plt.title('Operator')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.plot()\n",
    "    \n",
    "    #pout\n",
    "    plt.subplot(443)\n",
    "    x3=df['pout']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x3)\n",
    "    plt.title('Pout')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(444)\n",
    "    x4=df['hosp_ed']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Hospitaled')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(449)\n",
    "    x4=df['AgeCat']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Age')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,10)\n",
    "    x4=df['Gender']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Gender')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,11)\n",
    "    x4=df['LastContactN']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('LastContactN')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,12)\n",
    "    x4=df['LcdCat']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('LastContactDays')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "def eda2(df):\n",
    "    #max number of words in a sentence\n",
    "    df1 = pd.DataFrame(df)\n",
    "    df1['FreeText_count'] = df['FreeText'].apply(lambda x: Counter(x.split(' ')))\n",
    "    LEN = df1['FreeText_count'].apply(lambda x : sum(x.values()))\n",
    "    max_LEN = max(LEN)\n",
    "    global MAX_LEN\n",
    "    MAX_LEN = max_LEN\n",
    "    #length of sequence\n",
    "    df[\"FreeText_len\"] = df[\"FreeText\"].apply(lambda x: len(x))\n",
    "    #maximum number of sequence length\n",
    "    #print(df[\"FreeText_len\"].max())\n",
    "    df[\"FreeText_len\"].hist(figsize = (15, 10), bins = 100)\n",
    "    plt.show()\n",
    "    global MAX_SEQUENCE_LENGTH\n",
    "    MAX_SEQUENCE_LENGTH = df[\"FreeText_len\"].max()\n",
    "    global NB_WORDS\n",
    "    NB_WORDS = df['FreeText_len'].sum()\n",
    "    #word EDA\n",
    "    dummies2 = df.iloc[:,1:2]\n",
    "    tsne_plot(dummies2)\n",
    "    #wordcloud\n",
    "    df['FreeText'] = [cleaning(s) for s in df['FreeText']]\n",
    "    input_data = df['FreeText']\n",
    "    word_cloud(input_data)\n",
    "    with open('corpus.pickle', 'rb') as handle:\n",
    "        corpus = pickle.load(handle)\n",
    "    xt = np.concatenate(corpus)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    pd.value_counts(xt).plot(kind=\"barh\")\n",
    "    plt.show()\n",
    "    \n",
    "def word_cloud(input_data,title=None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 200,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(input_data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "#preprocessing function. use the dummies as input\n",
    "def pre_processing1(input,df):\n",
    "    sentence = [None] * df.shape[0]\n",
    "    for i in range(len(sentence)):\n",
    "        old_sentence = input.iloc[i]\n",
    "        word = list(old_sentence.split())\n",
    "        words = [None] * len(word)\n",
    "        for i in range(len(word)):\n",
    "            words[i] = re.sub(r'\\W+', '', word[i].lower())\n",
    "        words1 = [x for x in words if x is not None]\n",
    "        sentence.append(' '.join(words1))\n",
    "        sentence1 = [x for x in sentence if x is not None]\n",
    "    values = array(sentence1)\n",
    "    return values\n",
    "\n",
    "#this methods is courtesy from [1], it is an alternative preprocessing method that also uses stop words removal and normalization (in the main section)\n",
    "def pre_processing2(input):\n",
    "    #np.vectorize(input)\n",
    "    dummies1=re.sub(r\"\\w+\", \" \", input)\n",
    "    pattern = r\"[{}]\".format(\",.;\")\n",
    "    dummies1=re.sub(pattern, \"\", input)\n",
    "    #lower casing\n",
    "    dummies1= dummies1.lower()\n",
    "    dummies1 = dummies1.strip()\n",
    "    WPT = nltk.WordPunctTokenizer()\n",
    "    #tokenization\n",
    "    tokens = WPT.tokenize(dummies1)\n",
    "    #stop words removal\n",
    "    stop_word_list = nltk.corpus.stopwords.words('swedish')\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_word_list]\n",
    "    result = ' '.join(filtered_tokens)\n",
    "    return result\n",
    "\n",
    "#this function is used to transform string data into float data: e.g. Pout (String) to NewPout (float) using a scoring method where the highest value is the highest priority\n",
    "def transform_output_data(output_dataframe,datatype=None):\n",
    "    \n",
    "    #alternative 1\n",
    "    data2 = [None] * output_dataframe.shape[0]\n",
    "    data = output_dataframe\n",
    "    #float datatype by default\n",
    "    if datatype is None:\n",
    "        for i in range(len(data2)):\n",
    "            if data[i] == '1A':\n",
    "                data2.append(1.0)\n",
    "            elif data[i] == '1B':\n",
    "                data2.append(0.8)\n",
    "            elif data[i] == '2A':\n",
    "                data2.append(0.6)\n",
    "            elif data[i] == '2B':\n",
    "                data2.append(0.4)\n",
    "            else:\n",
    "                data2.append(0.2)\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "        data2 = np.array(data1)\n",
    "        df_data = pd.DataFrame({'NewPout': data2})\n",
    "        return df_data\n",
    "    elif datatype == 'int':\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == '1A':\n",
    "                data2.append(1)\n",
    "            elif data[i] == '1B':\n",
    "                data2.append(2)\n",
    "            elif data[i] == '2A':\n",
    "                data2.append(3)\n",
    "            elif data[i] == '2B':\n",
    "                data2.append(4)\n",
    "            else:\n",
    "                data2.append(5)\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "        data2 = np.array(data1)\n",
    "        df_data = pd.DataFrame({'NewPout': data2})\n",
    "        return df_data\n",
    "    elif datatype == 'multi':\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(output_dataframe)\n",
    "        label_encoded_y = label_encoder.transform(output_dataframe)\n",
    "        df_data = pd.DataFrame({'NewPout': label_encoded_y})\n",
    "        return df_data\n",
    "    #note to self: don't binarize the outputs, if there is binary outputs: use BoW for the binary outputs\n",
    "def inverse_transform_output_data(input):\n",
    "    data2 = [None] * input.shape[0]\n",
    "    data = input\n",
    "    for i in range(len(data2)):\n",
    "        if data[i] == 1.0 or data[i] == 1:\n",
    "            data2.append(\"1A\")\n",
    "        elif data[i] == 0.8 or data[i] == 2:\n",
    "            data2.append(\"1B\")\n",
    "        elif data[i] == 0.6 or data[i] == 3:\n",
    "            data2.append(\"2A\")\n",
    "        elif data[i] == 0.4 or data[i] == 4:\n",
    "            data2.append(\"2B\")\n",
    "        else:\n",
    "            data2.append(\"Referral\")\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "    data2 = np.array(data1)\n",
    "    df_data = pd.DataFrame({'NewPout': data2})\n",
    "    return df_data\n",
    "def text_processing(input_data, output_data, processing_method=None, truncated=None,datatype=None):\n",
    "    #bag-of-words for the none clause\n",
    "    if processing_method == None:\n",
    "        #one of these alternatives can be used but it depends on the classification result\n",
    "        #[2]\n",
    "        #Alternative 1 from [2]\n",
    "        #try to use aside from word: char or char_wb\n",
    "        bag_of_words_vector = CountVectorizer(analyzer=\"word\")\n",
    "        bag_of_words_matrix = bag_of_words_vector.fit_transform(input_data)\n",
    "        #denna är viktig\n",
    "        bag_of_words_matrix = bag_of_words_matrix.toarray()\n",
    "        '''\n",
    "        #Alternative 2\n",
    "        bag_of_words_vector = CountVectorizer(min_df = 0.0, max_df = 1.0, ngram_range=(2,2))\n",
    "        bag_of_words_matrix = bag_of_words_vector.fit_transform(input_data)\n",
    "        #denna är viktig\n",
    "        bag_of_words_matrix = bag_of_words_matrix.toarray()\n",
    "        '''\n",
    "        #using LSA: Latent Semantic Analysis or LSI\n",
    "        if truncated == 1:\n",
    "            svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "            truncated_bag_of_words = svd.fit_transform(bag_of_words_matrix)\n",
    "            #you can swap bag_of_words with truncated_bag_of_words\n",
    "            result = feature_engineering(truncated_bag_of_words,output_data)\n",
    "        result = feature_engineering(bag_of_words_matrix,output_data)\n",
    "        return result\n",
    "    elif processing_method =='tfidf':\n",
    "        \n",
    "        #[2]\n",
    "        Tfidf_Vector = TfidfVectorizer(analyzer=\"char_wb\")    \n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        '''\n",
    "        #Alternative 2 from [1]\n",
    "        Tfidf_Vector = TfidfVectorizer(min_df = 0., max_df = 1., use_idf = True)\n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        \n",
    "        \n",
    "        #Alternative 3\n",
    "        Tfidf_Vector = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1,1), sublinear_tf=True)\n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        '''\n",
    "       \n",
    "        \n",
    "        if truncated == 1:\n",
    "            svd2 = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "            #do we need to truncate the matrix?\n",
    "            #do we need to transform Tfidf_Matrix to an array before truncation?\n",
    "            truncated_tfidf = svd2.fit_transform(Tfidf_Matrix)\n",
    "            result = feature_engineering(truncated_tfidf,output_data)\n",
    "        #try to use truncated_tfidf instead tfidf_Matrix to see what happens\n",
    "        result = feature_engineering(Tfidf_Matrix,output_data)\n",
    "        return result\n",
    "    elif processing_method == 'onehot':\n",
    "        #be warned: one hot encoding only work well with binary outputs\n",
    "        #originates from [3]\n",
    "        label_encoder_input = LabelEncoder()\n",
    "        #label_encoder_output = LabelEncoder()\n",
    "        print(output_data.shape)\n",
    "        output1 = output_data.to_numpy()\n",
    "        array1 = [None] * input_data.shape[0]\n",
    "        for i in range(len(array1)):\n",
    "            input = input_data[i].split()\n",
    "            \n",
    "            values = array(input)\n",
    "            values1 = [x for x in values if x is not None]\n",
    "            array1.append(values1)\n",
    "        array2 = [x for x in array1 if x is not None]\n",
    "        array3 = array(array2)\n",
    "        array4 = np.hstack(array3)\n",
    "        array4.reshape(-1,len(output1.shape))\n",
    "        #output1 = output1.reshape(array4.shape)\n",
    "        #print(array4)\n",
    "        \n",
    "        integer_encoded_input = label_encoder_input.fit_transform(array4)\n",
    "        \n",
    "        #integer_encoded_output = label_encoder_output.fit_transform(output_data)\n",
    "        #float by default\n",
    "        if datatype is None:\n",
    "            #this method performs one hot encoding to return data of type float\n",
    "            onehot_encoder_input = OneHotEncoder(sparse=False)\n",
    "\n",
    "            #using reshaping before encoding\n",
    "            integer_encoded_input = integer_encoded_input.reshape(-1, 1)\n",
    "            encoded_input = onehot_encoder_input.fit_transform(integer_encoded_input)\n",
    "            \n",
    "            output= transform_output_data(output_data,'multi')\n",
    "            output1 = output.to_numpy()\n",
    "            \n",
    "           \n",
    "            #encoded_output = onehot_encoder_output.fit_transform(integer_encoded_output)\n",
    "        if datatype == 'int':\n",
    "            input_lb = LabelBinarizer()\n",
    "            encoded_input = input_lb.fit_transform(integer_encoded_input)\n",
    "            print(encoded_input)\n",
    "            print(encoded_input.shape)\n",
    "        #create training and test data using our encoded data\n",
    "        #change from integer_encoded_output to output_data\n",
    "        result = feature_engineering(encoded_input,output1)\n",
    "        \n",
    "        return result\n",
    "#split data into train and test data\n",
    "def feature_engineering(input_data, output_data):\n",
    "    #alternative 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.3, random_state=37)\n",
    "    '''\n",
    "    #alternative 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.3)\n",
    "    '''\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=37)\n",
    "    '''\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    result = [X_train, X_test, y_train, y_test]\n",
    "    return result\n",
    "def predictor(data_array,method,multiclass):\n",
    "    if method == 'NBG':\n",
    "        NBGres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        NBGres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [NBGres_BoW,NBGres_tfidf]\n",
    "        return result\n",
    "    elif method == 'NBM':\n",
    "        NBMres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        NBMres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [NBMres_BoW,NBMres_tfidf]\n",
    "        return result\n",
    "    elif method == 'SVM':\n",
    "        SVMres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        SVMres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [SVMres_BoW,SVMres_tfidf]\n",
    "        return result\n",
    "    elif method == 'RF':\n",
    "        RFres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        RFres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [RFres_BoW,RFres_tfidf]\n",
    "        return result\n",
    "    elif method == 'ensemble':\n",
    "        res_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        res_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [res_BoW,res_tfidf]\n",
    "        return result\n",
    "    elif method == 'GB':\n",
    "        GBres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        GBres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [GBres_BoW,GBres_tfidf]\n",
    "        return result\n",
    "    else:\n",
    "        logres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        logres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [logres_BoW,logres_tfidf]\n",
    "        #pred_prob is at index 4\n",
    "    return result\n",
    "#[6] prediction using the processed data\n",
    "def generate_metrics(result,clf=None):\n",
    "    title = ['Bow','TFIDF']\n",
    "    if clf == 'NBG':\n",
    "        val = 'Naive_Bayes_Gaussian'\n",
    "    elif clf == 'NBM':\n",
    "        val = 'Naive_Bayes_Multinomial'\n",
    "    elif clf == 'SVM':\n",
    "        val = 'Support_Vector_Machine'\n",
    "    elif clf == 'RF':\n",
    "        val = 'Random_Forest'\n",
    "    elif clf == 'ensemble':\n",
    "        val = 'Ensemble'\n",
    "    elif clf == 'gb':\n",
    "        val = 'Gradient_Boosting'\n",
    "    else:\n",
    "        val = 'Logistic_Regression'\n",
    "    print('Metrics from Bag of Words on '+ val +':')\n",
    "    print('-'*30)\n",
    "    result_from_predicitions(result[0])\n",
    "    print('-'*30)\n",
    "    print('Metrics from TF-IDF on '+ val +':')\n",
    "    print('-'*30)\n",
    "    result_from_predicitions(result[1])\n",
    "    print('-'*200)\n",
    "    plot_classification_report(result[0],result[1],title,val)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*200)\n",
    "    cm1 = metrics.confusion_matrix(y_true=result[0][0], y_pred=result[0][1])\n",
    "    print(cm1)\n",
    "    print('-'*200)\n",
    "    cm2 = metrics.confusion_matrix(y_true=result[1][0], y_pred=result[1][1])\n",
    "    print(cm2)\n",
    "    plot_cm(cm1,cm2,val)\n",
    "    print('-'*200)\n",
    "def train_predict_model(classifier,X_train,X_test, y_train, y_test,multiclass):\n",
    "    # build model\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    if classifier == 'NBG':\n",
    "        model = GaussianNB()\n",
    "    elif classifier == 'NBM':\n",
    "        model = MultinomialNB()\n",
    "    elif classifier == 'SVM':\n",
    "        model = LinearSVC()\n",
    "    elif classifier == 'RF':\n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "    elif classifier == 'ensemble':\n",
    "        model1 = LogisticRegression()\n",
    "        model2 = MultinomialNB()\n",
    "        model3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "        model = VotingClassifier(estimators=[('lr', model1), ('nb', model2), ('rf', model3)], voting='hard')\n",
    "    elif classifier == 'GB':\n",
    "        model = XGBClassifier(n_estimators=100)\n",
    "    else:\n",
    "        if multiclass == 'yes':\n",
    "            model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
    "        else:\n",
    "            model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    # predict using model\n",
    "    predicted = model.predict(X_test)\n",
    "    if (classifier != 'SVM' or classifier != 'ensemble'):\n",
    "        pred_prob = model.predict_proba(X_test)[:,1]\n",
    "        #pred_prob = None\n",
    "    else:\n",
    "        pred_prob = None\n",
    "    acc = metrics.accuracy_score(y_test,predicted)\n",
    "    acc = acc*100\n",
    "    if classifier == None:\n",
    "        loss = log_loss(y_test,predicted)\n",
    "        result = [predicted,acc,loss,pred_prob]\n",
    "    else:\n",
    "    \n",
    "        result = [predicted,acc,pred_prob]\n",
    "    return result    \n",
    "def initiate_predictions(train_test_data,method,multiclass):\n",
    "    X_train = train_test_data[0]\n",
    "    X_test = train_test_data[1]\n",
    "    y_train = train_test_data[2]\n",
    "    y_test = train_test_data[3]\n",
    "    prediction = train_predict_model(method,X_train,X_test,y_train,y_test,multiclass)\n",
    "    predicted = prediction[0]\n",
    "    acc = prediction[1]\n",
    "    true = y_test\n",
    "    \n",
    "    pred_prob = prediction[2]\n",
    "    if method == None:\n",
    "        loss = prediction[2]\n",
    "        pred_prob = prediction[3]\n",
    "        result = [true,predicted,acc,loss,pred_prob]\n",
    "    else:\n",
    "        result = [true,predicted,acc,pred_prob]\n",
    "    return result\n",
    "def plot_cm(cm1,cm2,method):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.imshow(cm1, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm1[i][j]))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    plt.imshow(cm2, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm2[i][j]))\n",
    "    plt.plot()\n",
    "    plt.savefig('confusionmatrix'+method+'.jpg')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def result_from_predicitions(prediction_array):\n",
    "    \n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*30)\n",
    "    df1=pd.DataFrame({'Actual':prediction_array[0], 'Predicted':prediction_array[1]})\n",
    "    print(df1)\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(prediction_array[0],prediction_array[1]),4))\n",
    "    print('Precision:', np.round(metrics.precision_score(prediction_array[0],prediction_array[1],average='weighted'),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(prediction_array[0],prediction_array[1],average='weighted'),4))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(prediction_array[0],prediction_array[1],average='weighted'),4))\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    print(metrics.classification_report(prediction_array[0],prediction_array[1]))\n",
    "    \n",
    "#AUCROC for binary class only\n",
    "def plot_roc(result1,result2,title,method):\n",
    "    #courtesy of DATAI https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv\n",
    "    # plot arrows, why? to present accuracy\n",
    "    fig1 = plt.figure(figsize=[20,10])\n",
    "    ax1 = fig1.add_subplot(121,aspect = 'equal')\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    #for i in range(len(result1[3])):\n",
    "    fpr, tpr, _ = roc_curve(result1[0], result1[3])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label=' (AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='red',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC '+title[0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "\n",
    "    ax2 = fig1.add_subplot(122,aspect = 'equal')\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    #for i in range(len(result2[3])):\n",
    "    fpr, tpr, _ = roc_curve(result2[0], result2[3])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label='(AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC ' + title[1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "    \n",
    "    plt.savefig('roc'+method+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#perform predictions on classification methods\n",
    "def clf_predictor(input_data,multiclass):\n",
    "    result_logregr = predictor(input_data,None,multiclass)\n",
    "    result_NBG = predictor(input_data,'NBGauss',multiclass)\n",
    "    result_NBM = predictor(input_data,'NBMulti',multiclass)\n",
    "    result_SVM = predictor(input_data,'SVM',multiclass)\n",
    "    result_RF = predictor(input_data,'RF',multiclass)\n",
    "    result_ensemble = predictor(input_data,'ensemble',multiclass)\n",
    "    result_GB = predictor(input_data,'gb',multiclass)\n",
    "    print(\"Evaluation of predictions:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_logregr)\n",
    "    #plots the graph for accuracies over different encoding metods for one particular classification method\n",
    "    print(\"Logistic Regression:\")\n",
    "    print(\"----------------------------------\")\n",
    "    result_logres_acc = [result_logregr[0][2],result_logregr[1][2]]\n",
    "    result_logres_loss = [result_logregr[0][3],result_logregr[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    if multiclass == 'no':\n",
    "        print(\"Naive Bayes (Gaussian):\")\n",
    "        generate_metrics(result_NBG,'NBG')\n",
    "        result_NBG_acc = [result_NBG[0][2],result_NBG[1][2]]\n",
    "        result_NBG_loss = [result_NBG[0][3],result_NBG[1][3]]\n",
    "        print(\"----------------------------------\")\n",
    "    elif multiclass == 'yes':\n",
    "        print(\"Naive Bayes (Multinomial):\")\n",
    "        print(\"----------------------------------\")\n",
    "        generate_metrics(result_NBM,'NBM')\n",
    "        result_NBM_acc = [result_NBM[0][2],result_NBM[1][2]]\n",
    "        result_NBM_loss = [result_NBM[0][3],result_NBM[1][3]]\n",
    "        print(\"----------------------------------\")\n",
    "    print(\"Support Vector Machine:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_SVM,'SVM')\n",
    "    result_SVM_acc = [result_SVM[0][2],result_SVM[1][2]]\n",
    "    result_SVM_loss = [result_SVM[0][3],result_SVM[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Gradient Boosting\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_GB,'gb')\n",
    "    result_GB_acc = [result_GB[0][2],result_GB[1][2]]\n",
    "    result_GB_loss = [result_GB[0][3],result_GB[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Random Forest:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_RF,'RF')\n",
    "    result_RF_acc = [result_RF[0][2],result_RF[1][2]]\n",
    "    result_RF_loss = [result_RF[0][3],result_RF[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Ensemble of Logistic Regression, Naive Bayes (Multinomial) and Random Forest:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_ensemble,'ensemble')\n",
    "    result_ensemble_acc = [result_ensemble[0][2],result_ensemble[1][2]]\n",
    "    result_ensemble_loss = [result_ensemble[0][3],result_ensemble[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    result_acc = [result_logres_acc[0],result_logres_acc[1], \\\n",
    "    result_NBG_acc[0],result_NBG_acc[1], \\\n",
    "    result_NBM_acc[0],result_NBM_acc[1], \\\n",
    "    result_SVM_acc[0],result_SVM_acc[1], \\\n",
    "    result_RF_acc[0],result_RF_acc[1], \\\n",
    "    result_ensemble_acc[0],result_ensemble_acc[1],result_GB_acc[0],result_GB_acc[1]]\n",
    "\n",
    "    result_loss = [result_logres_loss[0],result_logres_loss[1], \\\n",
    "    result_NBG_loss[0],result_NBG_loss[1], \\\n",
    "    result_NBM_loss[0],result_NBM_loss[1], \\\n",
    "    result_SVM_loss[0],result_SVM_loss[1], \\\n",
    "    result_RF_loss[0],result_RF_loss[1], \\\n",
    "    result_ensemble_loss[0],result_ensemble_loss[1],result_GB_loss[0],result_GB_loss[1]]\n",
    "\n",
    "    proba_data = [result_logregr[0][4],result_logregr[1][4],result_NBG[0][4],result_NBG[1][4],\\\n",
    "        result_NBM[0][4],result_NBM[1][4],result_SVM[0][4],result_SVM[1][4],\\\n",
    "        result_GB[0][4],result_GB[1][4],result_RF[0][4],result_RF[1][4],\\\n",
    "        result_ensemble[0][4],result_ensemble[1][4]]\n",
    "    test_data = [result_logregr[0][1],result_logregr[1][1],result_NBG[0][1],result_NBG[1][1],\\\n",
    "        result_NBM[0][1],result_NBM[1][1],result_SVM[0][1],result_SVM[1][1],\\\n",
    "        result_GB[0][1],result_GB[1][1],result_RF[0][1],result_RF[1][1],\\\n",
    "        result_ensemble[0][1],result_ensemble[1][1]]\n",
    "  \n",
    "    result1 = [result_acc, result_loss]\n",
    "    result2 = [proba_data,test_data]\n",
    "    result=[result1,result2]\n",
    "    return result\n",
    "#perform word embeddings inputs: dataframe input data and output data; output: embedded data such as X train and test and y train and test\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('swedish')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        \n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "#category is binary by default\n",
    "def word_embeddings(input_data, output_data,ANN,dense,el,category=None):\n",
    "    \n",
    "    #Don't use stop words removal for deep learning\n",
    "    #input_data = input_data.apply(remove_stopwords)\n",
    "    \n",
    "    data = feature_engineering(input_data, output_data)\n",
    "    '''\n",
    "    global model2\n",
    "    resmod2 = [None] * input_data.shape[0]\n",
    "    #print(np.array(input_data)[0])\n",
    "    for i in range(input_data.shape[0]):\n",
    "        resmod = model2.predict(np.array(input_data)[i])\n",
    "        print(resmod[0])\n",
    "        lst = list(resmod)\n",
    "        if lst[0] == \"('label_1',)\":\n",
    "            lst[0] = 1\n",
    "        else:\n",
    "            lst[0] = 0\n",
    "        \n",
    "        resmod2.append(tuple(lst))\n",
    "    print(resmod2)\n",
    "    resmod = [value for x in resmod2 for value in x and value is not None]\n",
    "    resmod = np.array(resmod2)\n",
    "    '''\n",
    "\n",
    "\n",
    "    print(\"Train set has total {0} entries with {1:.2f}% 0, {2:.2f}% 1\".format(len(data[0]),\n",
    "                                                                             (len(data[0][data[2] == 0]) / (len(data[0])*1.))*100,\n",
    "                                                                        (len(data[0][data[2] == 1]) / (len(data[0])*1.))*100))\n",
    "    print(\"Test set has total {0} entries with {1:.2f}% 0, {2:.2f}% 1\".format(len(data[1]),\n",
    "                                                                             (len(data[1][data[3] == 0]) / (len(data[1])*1.))*100,\n",
    "                                                                            (len(data[1][data[3] == 1]) / (len(data[1])*1.))*100))\n",
    "    \n",
    "    data_out = we_output_data_transform(data[2],data[3])\n",
    "    '''\n",
    "    if category == None:\n",
    "        data = feature_engineering(input_data, output_data)\n",
    "        data_out = we_output_data_transform(data[2],data[3])\n",
    "    else:\n",
    "        with open('df.pickle', 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "        count = len(df.index)\n",
    "        data = feature_engineering(input_data, output_data)\n",
    "        dataset,data_out = we_output_data_transform(data[2],data[3],'multi')\n",
    "        data_in1 = multivectorizer(dataset)\n",
    "        data_in2 = feature_engineering(data_in1[0], data_out[:count])\n",
    "        #validation data\n",
    "        data2 = feature_engineering(data_in2[0], data_out[0])\n",
    "    '''\n",
    "    \n",
    "    #index 0 = X_train\n",
    "    #index 1 = X_test\n",
    "    #index 2 = y_train\n",
    "    #index 3 = y_test\n",
    "    assert data[0].shape[0] == data[2].shape[0]\n",
    "    assert data[1].shape[0] == data[3].shape[0]\n",
    "    data_in1 = tokenizer(input_data,data[0], data[1])\n",
    "    \n",
    "    print(data_in1[2])\n",
    "    data_in2 = padding(data_in1[0], data_in1[1],input_data)\n",
    "    global MAX_SEQUENCE_LENGTH\n",
    "    MAX_SEQUENCE_LENGTH = data_in2[0].shape[1]\n",
    "    '''\n",
    "    data_in21 = vectorize_sequences(input_data)\n",
    "    '''\n",
    "    \n",
    "    #create validation data \n",
    "    data2 = feature_engineering(data_in2[0], data_out[0])\n",
    "    #data2[0] = X_val\n",
    "    #data2[2] = y_val\n",
    "    assert data2[1].shape[0] == data2[3].shape[0]\n",
    "    assert data2[0].shape[0] == data2[2].shape[0]\n",
    "    #fasttext (word_to_vec_map, word_to_index, index_to_words, vocab_size, dim)\n",
    "    #tip: try to swap the sv.vec file with cc.sv.300.vec\n",
    "    #load fasttext data into cnn1\n",
    "    #save\n",
    "    with open('data_in2.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_in2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data_out.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_out, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    '''\n",
    "    with open('data_in.pickle', 'rb') as handle:\n",
    "        data_in = pickle.load(handle)\n",
    "    with open('data_out.pickle', 'rb') as handle:\n",
    "        data_out = pickle.load(handle)\n",
    "    '''\n",
    "   \n",
    "    corpus = load_vectors2('./data/fasttext/sv.vec')\n",
    "    #data_train = sentences_to_indices(data[0],corpus[1],len(data_in1[2]))\n",
    "    #data_test = sentences_to_indices(data[1],corpus[1],len(data_in1[2]))\n",
    "    '''\n",
    "    wiki_news = './cc.sv.300.vec'\n",
    "    embed_fasttext = load_embed(wiki_news)\n",
    "    #step1 build vocab\n",
    "    vocab = build_vocab(input_data)\n",
    "    #vocab is your embedding matrix\n",
    "    #step2 check coverage\n",
    "    print(\"FastText : \")\n",
    "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "    print(oov_fasttext[:18])\n",
    "    '''\n",
    "    vocab = None\n",
    "    embedding_layer1 = pretrained_embedding_layer(corpus[0], corpus[1],vocab)\n",
    "    print(corpus[4])\n",
    "    \n",
    "    embedding_layer0=load_vectors_word2vec('./data/word2vec/sv.bin',data_in1[2])\n",
    "    \n",
    "    embedding_layer = [embedding_layer0,embedding_layer1]\n",
    "    #second preprocessing method\n",
    "    #change data_in2[0].shape[1] with (MAX_LEN,)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(date_time(1))\n",
    "    model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_in2[0],data_out[0],data2[0],data2[2],dense)\n",
    "    #or\n",
    "    #model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_train,data_out[0],None,None,dense)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print(\"\\nElapsed Time: \" + elapsed_time)\n",
    "    print(\"Completed Model Trainning\", date_time(1))\n",
    "    '''\n",
    "    with open('model'+ANN+'.pickle', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    '''\n",
    "    #data_in2[0] = X_train\n",
    "    #data[2] = y_train\n",
    "    return [data_in2, data2, data,model]\n",
    "def vectorize_sequences(sequences, dimension=4900):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "def multivectorization(concated):\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(concated['freetext'].values)\n",
    "    sequences = tokenizer.texts_to_sequences(concated['freetext'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "    result = [X,word_index]\n",
    "    return result\n",
    "def we_evaluation(model,model1,data1,data2,data3,data4,ANN1,ANN2 ,datax,datay):\n",
    "    \n",
    "    preds1 = model.predict(data1[1],batch_size=13)\n",
    "    preds2 = model1.predict(data3[1],batch_size=13)\n",
    "    preds1 = np.argmax(preds1, axis=-1)\n",
    "    preds2 = np.argmax(preds2, axis=-1)\n",
    "    with open('preds1.pickle', 'wb') as handle:\n",
    "        pickle.dump(preds1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('preds2.pickle', 'wb') as handle:\n",
    "        pickle.dump(preds2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('test_data1.pickle', 'wb') as handle:\n",
    "        pickle.dump(datax[3], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('test_data2.pickle', 'wb') as handle:\n",
    "        pickle.dump(datay[3], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #or\n",
    "    #preds2 = model.predict_classes(data_in2[1])\n",
    "    '''\n",
    "    Call the metrics function\n",
    "    '''\n",
    "\n",
    "  \n",
    "    test1 = ANN1\n",
    "    test2 = ANN2\n",
    "    title = [test1,test2]\n",
    "    #keras evaluation\n",
    "    score = model.evaluate(data2[1], data2[3], verbose=0)\n",
    "    print(\"Model Performance of model 1: \"+ test1 +\" (Test):\")\n",
    "    df_score1=pd.DataFrame.from_records([{'Accuracy':score[1],'Precision':score[2],'Recall':score[3],'F1_score':score[4]}])\n",
    "    print(df_score1)\n",
    "    score = model1.evaluate(data4[1], data4[3], verbose=0)\n",
    "    print(\"Model Performance of model 2: \"+ test2 +\" (Test):\")\n",
    "    df_score2=pd.DataFrame.from_records([{'Accuracy':score[1],'Precision':score[2],'Recall':score[3],'F1_score':score[4]}])\n",
    "    print(df_score2)\n",
    "    '''\n",
    "    result1 = pd.DataFrame({'model': test1, 'score': accuracy1[1]*100}, index=[-1])\n",
    "    result2 = pd.DataFrame({'model': test2, 'score': accuracy2[1]*100}, index=[-1])\n",
    "    result = pd.concat([result2, result1.ix[:]]).reset_index(drop=True)\n",
    "    plot_model_performace(result)\n",
    "    '''\n",
    "    #load pickle\n",
    "    with open('preds1.pickle', 'rb') as handle:\n",
    "        preds1 = pickle.load(handle)\n",
    "    \n",
    "    with open('preds2.pickle', 'rb') as handle:\n",
    "        preds2 = pickle.load(handle)\n",
    "    \n",
    "    with open('test_data1.pickle', 'rb') as handle:\n",
    "        test_data1 = pickle.load(handle)\n",
    "    \n",
    "    with open('test_data2.pickle', 'rb') as handle:\n",
    "        test_data2 = pickle.load(handle)\n",
    "   \n",
    "    '''\n",
    "    test_data1 = pd.Series(test_data1)\n",
    "    test_data2 = pd.Series(test_data2)\n",
    "    '''\n",
    "    \n",
    "    #SKLearn Evaluation\n",
    "    target_class = ['class_0','class_1']\n",
    "    labels = [0,1]\n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*200)\n",
    "    df1=pd.DataFrame({'Actual':test_data1, 'Predicted':preds1})\n",
    "    print(df1)\n",
    "    print('-'*200)\n",
    "    df2=pd.DataFrame({'Actual':test_data2, 'Predicted':preds2})\n",
    "    print(df2)\n",
    "    print('-'*200)\n",
    "    print(metrics.classification_report(test_data1,preds1,labels,target_class))\n",
    "    print('-'*200)\n",
    "    print(metrics.classification_report(test_data2,preds2,labels,target_class))\n",
    "    array1 = [test_data1,preds1]\n",
    "    array2 = [test_data2,preds2]\n",
    "    method = test1+'_'+test2\n",
    "    plot_classification_report(array1,array2,title,method)\n",
    "    print('-'*200)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*200)\n",
    "    cm1 = metrics.confusion_matrix(y_true=test_data1, y_pred=preds1)\n",
    "    print(cm1)\n",
    "    cm2 = metrics.confusion_matrix(y_true=test_data2, y_pred=preds2)\n",
    "    print(cm2)\n",
    "    plot_cm(cm1,cm2,method)\n",
    "    df1.to_csv('prediction1.csv', encoding='utf-8', index=True)\n",
    "    df2.to_csv('prediction2.csv', encoding='utf-8', index=True)\n",
    "def sentences_to_indices(X, word_to_index, maxLen):\n",
    "    m = X.shape[0] \n",
    "    X = np.array(X)\n",
    "    X_indices = np.zeros((m, maxLen))\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().strip().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index:\n",
    "                w = \"person\"  \n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices\n",
    "def plot_function(track):\n",
    "    plt.subplot(221)\n",
    "    plt.plot(track.history['acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(track.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.show()\n",
    "def plot_classification_report(array1, array2,title,method, ax=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(211)\n",
    "    \n",
    "    plt.title('Classification Report '+title[0])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array1[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array1[0], array1[1])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    \n",
    "    plt.title('Classification Report '+title[1])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array2[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array2[0], array2[1])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "    \n",
    "    \n",
    "    plt.savefig('classificationreport'+ method +'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#tokenizes the words\n",
    "def tokenizer2(train_data, test_data):\n",
    "    train = fastText.tokenize(train_data)\n",
    "    test = fastText.tokenize(test_data)\n",
    "    result = [train,test]\n",
    "    return result\n",
    "def tokenizer(input_data,train_data, test_data):\n",
    "    #from [7]\n",
    "    seq_lengths = input_data.apply(lambda x: len(x.split(' ')))\n",
    "    print(seq_lengths.describe())\n",
    "    \n",
    "    \n",
    "    max_sequence_len = max([len(x) for x in input_data])\n",
    "    print(max_sequence_len)\n",
    "    \n",
    "    tk = Tokenizer(num_words=NB_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \")\n",
    "    tk.fit_on_texts(input_data)\n",
    "    trained_seq = tk.texts_to_sequences(train_data)\n",
    "    test_seq = tk.texts_to_sequences(test_data)\n",
    "    word_index = tk.word_index\n",
    "    result = [trained_seq, np.array(test_seq), word_index]\n",
    "    return result\n",
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n",
    "#test function from [7] to make sure that the sequences generated from the tokenizer function are of equal length\n",
    "def test_sequence(train_data):\n",
    "    seq_lengths = train_data.apply(lambda x: len(x.split(' ')))\n",
    "    print(\"The sequences generated are:\")\n",
    "    seq_lengths.describe()\n",
    "    print(\"----------------\")\n",
    "def date_time(x):\n",
    "    if x==1:\n",
    "        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==2:    \n",
    "        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==3:  \n",
    "        return 'Date now: %s' % datetime.datetime.now()\n",
    "    if x==4:  \n",
    "        return 'Date today: %s' % datetime.date.today() \n",
    "def plot_performance(history=None, figure_directory=None, ylim_pad=[0, 0]):\n",
    "    xlabel = 'Epoch'\n",
    "    legends = ['Training', 'Validation']\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    y1 = history.history['f1_score']\n",
    "    y2 = history.history['val_f1_score']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[0]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[0]\n",
    "\n",
    "    plt.subplot(221)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model F1_score\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('F1_score', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['loss']\n",
    "    y2 = history.history['val_loss']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(222)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Loss', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['precision']\n",
    "    y2 = history.history['val_precision']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(223)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Precision\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Precision', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['recall']\n",
    "    y2 = history.history['val_recall']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(224)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Recall\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Recall', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "    if figure_directory:\n",
    "        plt.savefig(figure_directory+\"/history\")\n",
    "\n",
    "    plt.show()\n",
    "#in [7] padding is used to fill out null values\n",
    "def padding(trained_seq, test_seq,input):\n",
    "    \n",
    "    \n",
    "    trained_seq_trunc = pad_sequences(trained_seq,maxlen=MAX_LEN)\n",
    "    \n",
    "    test_seq_trunc = pad_sequences(test_seq,maxlen=MAX_LEN)\n",
    "    print(trained_seq_trunc.shape)\n",
    "    print(test_seq_trunc.shape)\n",
    "    result = [trained_seq_trunc, test_seq_trunc]\n",
    "    return result\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == './cc.sv.300.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "def we_output_data_transform(y_train,y_test,encoding='binary',category=None):\n",
    "    if encoding == 'multi':\n",
    "        '''\n",
    "        From Peter Nagy, Kaggle, https://www.kaggle.com/ngyptr/multi-class-classification-with-lstm\n",
    "        '''\n",
    "        with open('df.pickle', 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "        df1 = df['freetext']\n",
    "        df2 = df['pout']\n",
    "        frames = [df1,df2]\n",
    "        data = pd.concat(frames, axis=1, sort=False)\n",
    "        num_of_categories = df.count+1\n",
    "        shuffled = data.reindex(np.random.permutation(data.index))\n",
    "        prio1A = shuffled[shuffled['pout'] == '1A'][:num_of_categories]\n",
    "        prio1B = shuffled[shuffled['pout'] == '1B'][:num_of_categories]\n",
    "        prio2A = shuffled[shuffled['pout'] == '2A'][:num_of_categories]\n",
    "        prio2B = shuffled[shuffled['pout'] == '2B'][:num_of_categories]\n",
    "        Referal = shuffled[shuffled['pout'] == 'Referal'][:num_of_categories]\n",
    "        concated = pd.concat([prio1A,prio1B,prio2A,prio2B,Referal], ignore_index=True)\n",
    "        concated = concated.reindex(np.random.permutation(concated.index))\n",
    "        concated['LABEL'] = 0\n",
    "        concated.loc[concated['pout'] == '1A', 'LABEL'] = 0\n",
    "        concated.loc[concated['pout'] == '1B', 'LABEL'] = 1\n",
    "        concated.loc[concated['pout'] == '2A', 'LABEL'] = 2\n",
    "        concated.loc[concated['pout'] == '2B', 'LABEL'] = 3\n",
    "        concated.loc[concated['pout'] == 'Referal', 'LABEL'] = 4\n",
    "        labels = to_categorical(concated['LABEL'], num_classes=5)\n",
    "        if 'pout' in concated.keys():\n",
    "            concated.drop(['pout'], axis=1)\n",
    "        return concated,labels\n",
    "    else:\n",
    "        le = LabelEncoder()\n",
    "        #y_train_le = le.fit_transform(y_train.values)\n",
    "        #y_test_le = le.fit_transform(y_test.values)\n",
    "        y_train = to_categorical(y_train.values).astype('float32')\n",
    "        y_test = to_categorical(y_test.values).astype('float32')\n",
    "    result = [y_train,y_test]\n",
    "    return result\n",
    "#embeddings layer\n",
    "def embeddings_layer(X_train_emb, X_valid_emb,y_train_emb,y_valid_emb,dense):\n",
    "    emb_model = models.Sequential()\n",
    "    emb_model.add(layers.Embedding(NB_WORDS, DIM, input_length=MAX_LEN))\n",
    "    emb_model.add(layers.Flatten())\n",
    "    emb_model.add(layers.Dense(dense, activation='softmax'))\n",
    "    emb_model.summary()\n",
    "    emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)\n",
    "    result = emb_history\n",
    "    return result\n",
    "#from [7]\n",
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=1)\n",
    "    \n",
    "    return history\n",
    "#load vec file from fasttext, from [8]\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    vocab_size, dim = map(int, fin.readline().split())\n",
    "    word_to_vec_map = {}\n",
    "    words = set()\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        words.add(tokens[0])\n",
    "        word_to_vec_map[tokens[0]] = np.array(tokens[1:], dtype=np.float64)\n",
    "    i = 1\n",
    "    words_to_index = {}\n",
    "    index_to_words = {}\n",
    "    for w in sorted(words):\n",
    "        words_to_index[w] = i\n",
    "        index_to_words[i] = w\n",
    "        i = i + 1\n",
    "    return word_to_vec_map, words_to_index, index_to_words, vocab_size, dim\n",
    "#caller function for load_vectors .vec file\n",
    "def load_vectors2(fname):\n",
    "    word_to_vec_map, words_to_index, index_to_words, vocab_size, dim = load_vectors(fname)\n",
    "    result = [word_to_vec_map, words_to_index, index_to_words, vocab_size, dim]\n",
    "    return result\n",
    "#from[8]\n",
    "#for Word2Vec input word2vec .bin file and word_index = tokenizer.word_index from tokenizer\n",
    "def load_vectors_word2vec(fname,word_index):\n",
    "    word_vectors = KeyedVectors.load(fname)\n",
    "    \n",
    "    vocabulary_size = min(MAX_NB_WORDS, len(word_index))+1\n",
    "    embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i>=NB_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "    del(word_vectors)\n",
    "    embedding_layer = Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "    embedding_layer.build(MAX_LEN)\n",
    "    return embedding_layer\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")\n",
    "#[8]\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index,embeddings_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    #emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    emb_dim = 300\n",
    "    '''\n",
    "    #or\n",
    "    emb_dim = 300\n",
    "    '''\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        '''\n",
    "        if index >= NB_WORDS: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: emb_matrix[index] = embedding_vector\n",
    "        '''\n",
    "    embedding_layer = Embedding(input_dim = vocab_len, output_dim = emb_dim,\n",
    "    mask_zero = False,input_length = MAX_SEQUENCE_LENGTH,trainable=False) \n",
    "\n",
    "    embedding_layer.build(MAX_LEN)\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "#bilstm\n",
    "def bilstm(input_shape,embedding_layer1,dense):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Bidirectional(LSTM(300)))\n",
    "    model.add(Dense(600,activation='elu'))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.009,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def cnn2(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='elu', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(300,return_sequences=True))\n",
    "    model.add(GRU(300))\n",
    "    model.add(Dense(512,activation='elu'))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def cnn1(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(300))\n",
    "    model.add(Dense(512,activation='elu'))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def gru_model(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(GRU(117))\n",
    "    #model.add(Dense(234))\n",
    "    #model.add(Dense(3))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "def lstm_model(input_shape,embedding_layer1,dense):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(LSTM(117))\n",
    "    #model.add(Dense(234,activation='elu',kernel_regularizer=regularizers.l2(1e-9),activity_regularizer=regularizers.l1(1e-9),bias_regularizer=regularizers.l2(0.01), kernel_constraint=maxnorm(3)))\n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.008,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "    #from https://www.kaggle.com/sakarilukkarinen/embedding-lstm-gru-and-conv1d/versions\n",
    "def gru_model2(input_shape,embedding_layer1,dense):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(GRU(lstm_out,kernel_initializer='random_uniform',dropout=0.00001,recurrent_dropout=0.00001,activation='elu',use_bias=True,return_sequences=True))\n",
    "    model.add(GRU(MAX_LEN,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(lstm_out,activation='elu'))\n",
    "    \n",
    "    #model.add(Dense(3,activation='elu'))\n",
    "    model.add(Dense(dense, use_bias=False,activation='softmax',kernel_regularizer=regularizers.l2(1e-16),activity_regularizer=regularizers.l1(1e-16),bias_regularizer=regularizers.l2(0.01), kernel_constraint=maxnorm(3)))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.009,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=adam,metrics = ['accuracy',precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def predict_model(input_shape,embedding_layer,model_type,X_train,y_train,X_val,y_val,dense):\n",
    "    callbacks = [EarlyStopping(monitor='val_loss',patience=2)]\n",
    "    #you can also use rmsprop as optimizer\n",
    "    #adam = Adam(lr=1e-3)\n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    #loss = 'categorical_crossentropy'\n",
    "    if model_type == 'cnn1':\n",
    "        model = cnn1((input_shape,),embedding_layer,dense)\n",
    "        #model.compile(loss=loss,optimizer=adam,metrics=['acc'])\n",
    "        #for train, test in kfold.split(X, Y):\n",
    "        track = model.fit(X_train, y_train, batch_size=13, epochs=20, verbose=1,validation_data=(X_val, y_val))\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        [9] https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7\n",
    "        By Ricky Kim, Another Twitter sentiment analysis with Python — Part 9 (Neural Networks with Tfidf vectors using Keras)\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'cnn2':\n",
    "        model = cnn2((input_shape,),embedding_layer,dense)\n",
    "        #model1.compile(loss=loss,optimizer=adam,metrics=['acc'])\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, batch_size=13, epochs=20, verbose=1,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track2)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'lstm':\n",
    "        model = lstm_model((input_shape,),embedding_layer,dense)\n",
    "        #The model has already been compiled in the function call\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'bilstm':\n",
    "        model = bilstm((input_shape,),embedding_layer,dense)\n",
    "        #The model has already been compiled in the function call\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "    elif model_type == 'gru':\n",
    "        model = gru_model((input_shape,),embedding_layer,dense)\n",
    "        #The model has already been compiled in the function call\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        plot_performance(track)\n",
    "        \n",
    "    else:\n",
    "        model = gru_model2((input_shape,),embedding_layer,dense)\n",
    "        #track = model.fit(X_train[train], y_train[train], batch_size=13, epochs=40, verbose=1,validation_data=(X_val, y_val))\n",
    "        track = model.fit(X_train, y_train, epochs=20, batch_size=13,verbose=1,shuffle=False,validation_data=(X_val, y_val))\n",
    "        '''\n",
    "        model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5,validation_data=(X_val, y_val),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)\n",
    "        '''\n",
    "        #plot_function(track)\n",
    "        '''\n",
    "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t    cvscores.append(scores[1] * 100)\n",
    "        '''\n",
    "        \n",
    "        plot_performance(track)\n",
    "    #print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "    #model = None\n",
    "    return model\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "def plot_model_performace(result):\n",
    "    \n",
    "    sns.set_style(\"ticks\")\n",
    "    figsize=(22, 6)\n",
    "\n",
    "    ticksize = 12\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    xlabel = \"Model\"\n",
    "    ylabel = \"Score\"\n",
    "\n",
    "    title = \"Model Performance\"\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "              'axes.labelsize' : labelsize,\n",
    "              'axes.titlesize' : titlesize,\n",
    "              'xtick.labelsize': ticksize,\n",
    "              'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "\n",
    "    col1 = \"model\"\n",
    "    col2 = \"score\"\n",
    "    sns.barplot(x=col1, y=col2, data=result)\n",
    "\n",
    "    plt.title(title.title())\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid()\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split), ignore_index=True)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "#to call the function above, do this: parallelize_dataframe(df, cleaning), then pickle\n",
    "#from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras, visited 26th may\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "'''\n",
    "[1] https://medium.com/deep-learning-turkey/text-processing-1-old-fashioned-methods-bag-of-words-and-tfxidf-b2340cc7ad4b, Medium, Deniz Kilinc visited 6th of April 2019\n",
    "[2] https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm, from ReiiNakano , Kaggle, visited 5th of April 2019\n",
    "[3] https://github.com/codebasics/py/tree/master/ML, github, Codebasics from dhavalsays, visited 6th of April 2019\n",
    "[4] from scikit-learn.org (base code), visited 4th of April 2019\n",
    "[5] Python One Hot Encoding with SciKit Learn, InsightBot, http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn, visited 6th April 2019 \n",
    "[6] Kaggle, Sentiment Analysis : CountVectorizer & TF-IDF, Divyojyoti Sinha, https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\n",
    "[7] Kaggle, Bert Carremans, Using Word Embeddings for Sentiment Analysis, https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis, visited april 11th 2019\n",
    "[8] Sentiment Analysis with pretrained Word2Vec, Varun Sharma, Kaggle, https://www.kaggle.com/varunsharmaml/sentiment-analysis-with-pretrained-word2vec, visited 12th of april 2019\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
