{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'talos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1749f92a3bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtalos\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtalos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmeasure_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtalos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talos'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "from IPython import get_ipython\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    ipy.run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import re\n",
    "import talos as ta\n",
    "from talos.metrics.keras_metrics import fmeasure_acc\n",
    "from talos import live\n",
    "import scipy as sp\n",
    "import io\n",
    "import gc\n",
    "from itertools import cycle\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from wordcloud import WordCloud\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ELU\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import learning_curve\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "import nltk\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.corpus import stopwords\n",
    "#import scikitplot.plotters as skplt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Activation, LeakyReLU, PReLU, ELU, ThresholdedReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import unit_norm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from gensim.models import FastText\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Conv1D, MaxPooling1D, Dropout, Concatenate, Conv2D, MaxPooling2D, concatenate,BatchNormalization, Bidirectional\n",
    "from keras.initializers import glorot_uniform\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.activations import relu\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import seaborn as sns\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from gensim.models.wrappers import FastText\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import datetime\n",
    "#import fastText\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import string\n",
    "import operator \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "num_partitions = multiprocessing.cpu_count()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "#Global Variables from [7] and others\n",
    "NB_WORDS = 4037069  # Parameter indicating the number of words we'll put in the dictionary \n",
    "\n",
    "VAL_SIZE = 9  # Size of the validation set (originally 1000)\n",
    "NB_START_EPOCHS = 8  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 134  # Maximum number of words in a sequence\n",
    "#MAX_LEN = 62\n",
    "GLOVE_DIM = 50  # Number of dimensions of the GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 860\n",
    "MAX_NB_WORDS = 4037069\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "#activation function\n",
    "act = 'relu'\n",
    "re_weight = True\n",
    "#dimension for fasttext\n",
    "DIM = 300\n",
    "#dimension for word2vec\n",
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "dim = 50\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "maxLen = 60\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "lstm_output_size = 70\n",
    "#model2 = fastText.train_supervised('./fasttext_train.txt',label='label_', epoch=20, dim=200)\n",
    "\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\s\\W',' ',s)\n",
    "    s = re.sub(r'\\W,\\s',' ',s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(r\"\\d+\", \"\", s)\n",
    "    s = re.sub(r'\\s+',' ',s)\n",
    "    s = re.sub(r'[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\",\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    \n",
    "    return s\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode('utf8', 'ignore')\n",
    "    return txt\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    data['FreeText'] = [cleaning(s) for s in data['FreeText']]\n",
    "    corpus = []\n",
    "    for col in ['FreeText']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "    \n",
    "    with open('data/corpus.pickle', 'wb') as handle:\n",
    "        pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return corpus\n",
    "def tsne_plot(df):\n",
    "    corpus = build_corpus(df)\n",
    "    \n",
    "    model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=1, workers=4)\n",
    "    with open('data/w2vmodel.pickle', 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #model.most_similar('')\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "def transformAge(df):\n",
    "    '''\n",
    "    This function will categorise the age data into 5 different age categories\n",
    "    '''\n",
    "    dfage = df.Age\n",
    "\n",
    "    array = []\n",
    "    for i in range(len(dfage)):\n",
    "        if dfage[i] < 3:\n",
    "            array.append(0)\n",
    "        elif dfage[i] >= 3 and dfage[i] < 13:\n",
    "            array.append(1)\n",
    "        elif dfage[i] >= 13 and dfage[i] < 19:\n",
    "            array.append(2)\n",
    "        elif dfage[i] >= 19 and dfage[i] < 65:\n",
    "            array.append(3)\n",
    "        else:\n",
    "            array.append(4)\n",
    "    df[\"AgeCat\"] = array\n",
    "    return df\n",
    "def transformLCD(df):\n",
    "    '''\n",
    "    This function will transform the numerical LCD data into 3 categories\n",
    "    '''\n",
    "    dflcd = df.LastContactDays\n",
    "    array = []\n",
    "    for i in range(len(dflcd)):\n",
    "        if dflcd[i] < 2:\n",
    "            array.append(0)\n",
    "        elif dflcd[i] >= 2 and dflcd[i] < 31:\n",
    "            array.append(1)\n",
    "        else:\n",
    "            array.append(2)\n",
    "    df[\"LcdCat\"] = array\n",
    "    return df\n",
    "def test_code(input):\n",
    "    print(\"Hello \" + input)\n",
    "def eda1(df):\n",
    "    figsize=(20, 10)\n",
    "    \n",
    "    ticksize = 14\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "            'axes.labelsize' : labelsize,\n",
    "            'axes.titlesize' : titlesize,\n",
    "            'xtick.labelsize': ticksize,\n",
    "            'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "    \n",
    "    plt.subplot(441)\n",
    "    x1=df['prio']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x1)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Priority')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "\n",
    "    plt.subplot(442)\n",
    "    x2=df['operator']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x2)\n",
    "   # plt.title(\"Review Sentiment Count\")\n",
    "    plt.title('Operator')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.plot()\n",
    "    \n",
    "    #pout\n",
    "    plt.subplot(443)\n",
    "    x3=df['pout']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x3)\n",
    "    plt.title('Pout')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(444)\n",
    "    x4=df['hosp_ed']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Hospitaled')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(449)\n",
    "    x4=df['AgeCat']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Age')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,10)\n",
    "    x4=df['Gender']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('Gender')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,11)\n",
    "    x4=df['LastContactN']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('LastContactN')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(4,4,12)\n",
    "    x4=df['LcdCat']\n",
    "    ylabel = \"Count\"\n",
    "    sns.countplot(x4)\n",
    "    plt.title('LastContactDays')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "def eda2(df):\n",
    "    #max number of words in a sentence\n",
    "    df1 = pd.DataFrame(df)\n",
    "    df1['FreeText_count'] = df['freetext'].astype(str).apply(lambda x: Counter(x.split(' ')))\n",
    "    LEN = df1['FreeText_count'].apply(lambda x : sum(x.values()))\n",
    "    max_LEN = max(LEN)\n",
    "    global MAX_LEN\n",
    "    MAX_LEN = max_LEN\n",
    "    print(\"MAX_LEN is:\")\n",
    "    print(max_LEN)\n",
    "    #length of sequence\n",
    "    df[\"FreeText_len\"] = df[\"freetext\"].astype(str).apply(lambda x: len(x))\n",
    "    #maximum number of sequence length\n",
    "    #print(df[\"FreeText_len\"].max())\n",
    "    df[\"FreeText_len\"].hist(figsize = (15, 10), bins = 100)\n",
    "    plt.show()\n",
    "    global MAX_SEQUENCE_LENGTH\n",
    "    MAX_SEQUENCE_LENGTH = df[\"FreeText_len\"].max()\n",
    "    print(\"MAX_SEQUENCE_LENGTH is:\")\n",
    "    print(MAX_SEQUENCE_LENGTH)\n",
    "    global NB_WORDS\n",
    "    NB_WORDS = df['FreeText_len'].sum()\n",
    "    print(\"NB_WORDS is:\")\n",
    "    print(NB_WORDS)\n",
    "    #word EDA\n",
    "    dummies2 = df.iloc[:,1:2]\n",
    "    #tsne_plot(dummies2)\n",
    "    #wordcloud\n",
    "    df['FreeText'] = [cleaning(s) for s in df['freetext'].astype(str)]\n",
    "    #input_data = df['freetext']\n",
    "    #word_cloud(input_data)\n",
    "    #with open('corpus.pickle', 'rb') as handle:\n",
    "        #corpus = pickle.load(handle)\n",
    "    #xt = np.concatenate(corpus)\n",
    "    #plt.figure(figsize=(20,10))\n",
    "    #pd.value_counts(xt).plot(kind=\"barh\")\n",
    "    #plt.show()\n",
    "#preprocessing function. use the dummies as input\n",
    "def pre_processing1(input,df):\n",
    "    sentence = [None] * df.shape[0]\n",
    "    for i in range(len(sentence)):\n",
    "        old_sentence = input.iloc[i]\n",
    "        word = list(old_sentence.split())\n",
    "        words = [None] * len(word)\n",
    "        for i in range(len(word)):\n",
    "            words[i] = re.sub(r'\\W+', '', word[i].lower())\n",
    "        words1 = [x for x in words if x is not None]\n",
    "        sentence.append(' '.join(words1))\n",
    "        sentence1 = [x for x in sentence if x is not None]\n",
    "    values = array(sentence1)\n",
    "    return values\n",
    "#this methods is courtesy from [1], it is an alternative preprocessing method that also uses stop words removal and normalization (in the main section)\n",
    "def pre_processing2(input):\n",
    "    #np.vectorize(input)\n",
    "    dummies1=re.sub(r\"\\w+\", \" \", input)\n",
    "    pattern = r\"[{}]\".format(\",.;\")\n",
    "    dummies1=re.sub(pattern, \"\", input)\n",
    "    #lower casing\n",
    "    dummies1= dummies1.lower()\n",
    "    dummies1 = dummies1.strip()\n",
    "    WPT = nltk.WordPunctTokenizer()\n",
    "    #tokenization\n",
    "    tokens = WPT.tokenize(dummies1)\n",
    "    #stop words removal\n",
    "    stop_word_list = nltk.corpus.stopwords.words('swedish')\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_word_list]\n",
    "    result = ' '.join(filtered_tokens)\n",
    "    return result\n",
    "#this function is used to transform string data into float data: e.g. Pout (String) to NewPout (float) using a scoring method where the highest value is the highest priority\n",
    "def transform_output_data(output_dataframe,datatype=None):\n",
    "    \n",
    "    #alternative 1\n",
    "    data2 = [None] * output_dataframe.shape[0]\n",
    "    data = output_dataframe\n",
    "    #float datatype by default\n",
    "    if datatype is None:\n",
    "        for i in range(len(data2)):\n",
    "            if data[i] == '1A':\n",
    "                data2.append(1.0)\n",
    "            elif data[i] == '1B':\n",
    "                data2.append(0.8)\n",
    "            elif data[i] == '2A':\n",
    "                data2.append(0.6)\n",
    "            elif data[i] == '2B':\n",
    "                data2.append(0.4)\n",
    "            else:\n",
    "                data2.append(0.2)\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "        data2 = np.array(data1)\n",
    "        df_data = pd.DataFrame({'NewPout': data2})\n",
    "        return df_data\n",
    "    elif datatype == 'int':\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == '1A':\n",
    "                data2.append(1)\n",
    "            elif data[i] == '1B':\n",
    "                data2.append(2)\n",
    "            elif data[i] == '2A':\n",
    "                data2.append(3)\n",
    "            elif data[i] == '2B':\n",
    "                data2.append(4)\n",
    "            else:\n",
    "                data2.append(5)\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "        data2 = np.array(data1)\n",
    "        df_data = pd.DataFrame({'NewPout': data2})\n",
    "        return df_data\n",
    "    elif datatype == 'multi':\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(output_dataframe)\n",
    "        label_encoded_y = label_encoder.transform(output_dataframe)\n",
    "        df_data = pd.DataFrame({'NewPout': label_encoded_y})\n",
    "        return df_data\n",
    "    #note to self: don't binarize the outputs, if there is binary outputs: use BoW for the binary outputs\n",
    "def inverse_transform_output_data(input):\n",
    "    data2 = [None] * input.shape[0]\n",
    "    data = input\n",
    "    for i in range(len(data2)):\n",
    "        if data[i] == 1.0 or data[i] == 1:\n",
    "            data2.append(\"1A\")\n",
    "        elif data[i] == 0.8 or data[i] == 2:\n",
    "            data2.append(\"1B\")\n",
    "        elif data[i] == 0.6 or data[i] == 3:\n",
    "            data2.append(\"2A\")\n",
    "        elif data[i] == 0.4 or data[i] == 4:\n",
    "            data2.append(\"2B\")\n",
    "        else:\n",
    "            data2.append(\"Referral\")\n",
    "            data1 = [x for x in data2 if x is not None]\n",
    "    data2 = np.array(data1)\n",
    "    df_data = pd.DataFrame({'NewPout': data2})\n",
    "    return df_data\n",
    "def text_processing(input_data, output_data, processing_method=None, truncated=None,datatype=None):\n",
    "    #bag-of-words for the none clause\n",
    "    if processing_method == None:\n",
    "        #one of these alternatives can be used but it depends on the classification result\n",
    "        #[2]\n",
    "        #Alternative 1 from [2]\n",
    "        '''\n",
    "        #try to use aside from word: char or char_wb\n",
    "        bag_of_words_vector = CountVectorizer(analyzer=\"word\")\n",
    "        bag_of_words_matrix = bag_of_words_vector.fit_transform(input_data)\n",
    "        #denna är viktig\n",
    "        bag_of_words_matrix = bag_of_words_matrix.toarray()\n",
    "        '''\n",
    "        #Alternative 2\n",
    "        bag_of_words_vector = CountVectorizer(min_df = 0.0, max_df = 1.0, ngram_range=(1,1))\n",
    "        bag_of_words_matrix = bag_of_words_vector.fit_transform(input_data)\n",
    "        #denna är viktig\n",
    "        bag_of_words_matrix = bag_of_words_matrix.toarray()\n",
    "        \n",
    "        #using LSA: Latent Semantic Analysis or LSI\n",
    "        if truncated == 1:\n",
    "            svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "            truncated_bag_of_words = svd.fit_transform(bag_of_words_matrix)\n",
    "            #you can swap bag_of_words with truncated_bag_of_words\n",
    "            result = feature_engineering(truncated_bag_of_words,output_data)\n",
    "        result = feature_engineering(bag_of_words_matrix,output_data)\n",
    "        return result\n",
    "    elif processing_method =='tfidf':\n",
    "        '''\n",
    "        #[2]\n",
    "        Tfidf_Vector = TfidfVectorizer(analyzer=\"char_wb\")    \n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        \n",
    "        #Alternative 2 from [1]\n",
    "        Tfidf_Vector = TfidfVectorizer(min_df = 0., max_df = 1., use_idf = True)\n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "        '''\n",
    "        \n",
    "        #Alternative 3\n",
    "        Tfidf_Vector = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1,1), sublinear_tf=True)\n",
    "        Tfidf_Matrix = Tfidf_Vector.fit_transform(input_data)\n",
    "        Tfidf_Matrix = Tfidf_Matrix.toarray()\n",
    "    \n",
    "       \n",
    "        \n",
    "        if truncated == 1:\n",
    "            svd2 = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "            #do we need to truncate the matrix?\n",
    "            #do we need to transform Tfidf_Matrix to an array before truncation?\n",
    "            truncated_tfidf = svd2.fit_transform(Tfidf_Matrix)\n",
    "            result = feature_engineering(truncated_tfidf,output_data)\n",
    "        #try to use truncated_tfidf instead tfidf_Matrix to see what happens\n",
    "        result = feature_engineering(Tfidf_Matrix,output_data)\n",
    "        return result\n",
    "    elif processing_method == 'onehot':\n",
    "        #be warned: one hot encoding only work well with binary outputs\n",
    "        #originates from [3]\n",
    "        label_encoder_input = LabelEncoder()\n",
    "        #label_encoder_output = LabelEncoder()\n",
    "        print(output_data.shape)\n",
    "        output1 = output_data.to_numpy()\n",
    "        array1 = [None] * input_data.shape[0]\n",
    "        for i in range(len(array1)):\n",
    "            input = input_data[i].split()\n",
    "            \n",
    "            values = array(input)\n",
    "            values1 = [x for x in values if x is not None]\n",
    "            array1.append(values1)\n",
    "        array2 = [x for x in array1 if x is not None]\n",
    "        array3 = array(array2)\n",
    "        array4 = np.hstack(array3)\n",
    "        array4.reshape(-1,len(output1.shape))\n",
    "        #output1 = output1.reshape(array4.shape)\n",
    "        #print(array4)\n",
    "        \n",
    "        integer_encoded_input = label_encoder_input.fit_transform(array4)\n",
    "        \n",
    "        #integer_encoded_output = label_encoder_output.fit_transform(output_data)\n",
    "        #float by default\n",
    "        if datatype is None:\n",
    "            #this method performs one hot encoding to return data of type float\n",
    "            onehot_encoder_input = OneHotEncoder(sparse=False)\n",
    "\n",
    "            #using reshaping before encoding\n",
    "            integer_encoded_input = integer_encoded_input.reshape(-1, 1)\n",
    "            encoded_input = onehot_encoder_input.fit_transform(integer_encoded_input)\n",
    "            \n",
    "            output= transform_output_data(output_data,'multi')\n",
    "            output1 = output.to_numpy()\n",
    "            \n",
    "           \n",
    "            #encoded_output = onehot_encoder_output.fit_transform(integer_encoded_output)\n",
    "        if datatype == 'int':\n",
    "            input_lb = LabelBinarizer()\n",
    "            encoded_input = input_lb.fit_transform(integer_encoded_input)\n",
    "            print(encoded_input)\n",
    "            print(encoded_input.shape)\n",
    "        #create training and test data using our encoded data\n",
    "        #change from integer_encoded_output to output_data\n",
    "        result = feature_engineering(encoded_input,output1)\n",
    "        \n",
    "        return result\n",
    "#split data into train and test data\n",
    "def feature_engineering(input_data, output_data):\n",
    "    #alternative 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=None)\n",
    "    '''\n",
    "    #alternative 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.3)\n",
    "    '''\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=37)\n",
    "    '''\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    result = [X_train, X_test, y_train, y_test]\n",
    "    return result\n",
    "def predictor(data_array,method,multiclass):\n",
    "    if method == 'NBG':\n",
    "        NBGres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        NBGres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [NBGres_BoW,NBGres_tfidf]\n",
    "        return result\n",
    "    elif method == 'NBM':\n",
    "        NBMres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        NBMres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [NBMres_BoW,NBMres_tfidf]\n",
    "        return result\n",
    "    elif method == 'SVM':\n",
    "        SVMres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        SVMres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [SVMres_BoW,SVMres_tfidf]\n",
    "        return result\n",
    "    elif method == 'ensemble':\n",
    "        res_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        res_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [res_BoW,res_tfidf]\n",
    "        return result\n",
    "    elif method == 'RF':\n",
    "        RFres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        RFres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [RFres_BoW,RFres_tfidf]\n",
    "        return result\n",
    "    elif method == 'tree':\n",
    "        treeres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        treeres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [treeres_BoW,treeres_tfidf]\n",
    "        return result\n",
    "    elif method == 'GB':\n",
    "        GBres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        GBres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [GBres_BoW,GBres_tfidf]\n",
    "        return result\n",
    "    else:\n",
    "        logres_BoW = initiate_predictions(data_array[0],method,multiclass)\n",
    "        logres_tfidf = initiate_predictions(data_array[1],method,multiclass)\n",
    "        result = [logres_BoW,logres_tfidf]\n",
    "        #pred_prob is at index 4\n",
    "    return result\n",
    "#[6] prediction using the processed data\n",
    "def generate_metrics(result,clf=None):\n",
    "    title = ['Bow','TFIDF']\n",
    "    if clf == 'NBG':\n",
    "        val = ['Naive_Bayes_Gaussian','Naive_Bayes_Gaussian']\n",
    "    elif clf == 'NBM':\n",
    "        val = ['Naive_Bayes_Multinomial','Naive_Bayes_Multinomial']\n",
    "    elif clf == 'SVM':\n",
    "        val = ['Support_Vector_Machine','Support_Vector_Machine']\n",
    "    elif clf == 'RF':\n",
    "        val = ['Random_Forest','Random_Forest']\n",
    "    elif clf == 'ensemble':\n",
    "        val = ['Ensemble','Ensemble']\n",
    "    elif clf == 'GB':\n",
    "        val = ['Gradient_Boosting','Gradient_Boosting']\n",
    "    elif clf == 'tree':\n",
    "        val = ['Decision Tree','Decision Tree']\n",
    "    else:\n",
    "        val = ['Logistic_Regression','Logistic_Regression']\n",
    "    print('Metrics from Bag of Words on '+ val +':')\n",
    "    print('-'*30)\n",
    "    result_from_predicitions(result[0],title[0])\n",
    "    print('-'*30)\n",
    "    print('Metrics from TF-IDF on '+ val +':')\n",
    "    print('-'*30)\n",
    "    result_from_predicitions(result[1],title[1])\n",
    "    print('-'*200)\n",
    "    plot_classification_report(result[0],result[1],title,val)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*200)\n",
    "    cm1 = metrics.confusion_matrix(y_true=result[0][0], y_pred=result[0][1])\n",
    "    print(cm1)\n",
    "    print('-'*200)\n",
    "    cm2 = metrics.confusion_matrix(y_true=result[1][0], y_pred=result[1][1])\n",
    "    print(cm2)\n",
    "    plot_cm(cm1,cm2,title,val)\n",
    "    print('-'*200)\n",
    "def train_predict_model(classifier,X_train,X_test, y_train, y_test,multiclass):\n",
    "    # build model\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    if classifier == 'NBG':\n",
    "        model = BernoulliNB()\n",
    "        print(model)\n",
    "    elif classifier == 'NBM':\n",
    "        model = MultinomialNB()\n",
    "    elif classifier == 'SVM':\n",
    "        model = SVC(kernel='linear',probability=True)\n",
    "        print(model)\n",
    "    elif classifier == 'RF':\n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "        print(model)\n",
    "    elif classifier == 'ensemble':\n",
    "        model1 = BernoulliNB()\n",
    "        model2 = LogisticRegression()\n",
    "        model3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "        model = VotingClassifier(estimators=[('nb', model1), ('lr', model2)], voting='soft')\n",
    "        print(model)\n",
    "    elif classifier == 'GB':\n",
    "        model = GradientBoostingClassifier()\n",
    "        print(model)\n",
    "    elif classifier == 'tree':\n",
    "        model = tree.DecisionTreeClassifier(random_state=0)\n",
    "        print(model)\n",
    "    else:\n",
    "        if multiclass == 'yes':\n",
    "            model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
    "        else:\n",
    "            model = LogisticRegression()\n",
    "            print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    # predict using model\n",
    "    predicted = model.predict(X_test)\n",
    "    if (classifier != 'SVM' or classifier != 'ensemble'):\n",
    "        pred_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    else:\n",
    "        pred_prob = None\n",
    "   \n",
    "    acc = metrics.accuracy_score(y_test,predicted)\n",
    "    acc = acc*100\n",
    "    if classifier == None:\n",
    "        loss = log_loss(y_test,predicted)\n",
    "        result = [predicted,acc,pred_prob,loss]\n",
    "    else:\n",
    "    \n",
    "        result = [predicted,acc,pred_prob]\n",
    "    return result    \n",
    "def initiate_predictions(train_test_data,method,multiclass):\n",
    "    X_train = train_test_data[0]\n",
    "    X_test = train_test_data[1]\n",
    "    y_train = train_test_data[2]\n",
    "    y_test = train_test_data[3]\n",
    "    prediction = train_predict_model(method,X_train,X_test,y_train,y_test,multiclass)\n",
    "    predicted = prediction[0]\n",
    "    acc = prediction[1]\n",
    "    #true = y_train\n",
    "    true = y_test\n",
    "    \n",
    "    pred_prob = prediction[2]\n",
    "    \n",
    "    result = [true,predicted,acc,pred_prob]\n",
    "    return result\n",
    "def plot_cm(cm1,cm2,method,title):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.imshow(cm1, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result using '+title[0])\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm1[i][j]))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    plt.imshow(cm2, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result using '+title[1])\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm2[i][j]))\n",
    "    plt.plot()\n",
    "    plt.savefig('confusion_matrix_'+method+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def plot_cm2(cm1,cm2,method):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.imshow(cm1, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result'+ method[0])\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm1[i][j]))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    plt.imshow(cm2, interpolation='nearest', cmap=plt.cm.get_cmap('Wistia'))\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Result' + method[1])\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames)\n",
    "    plt.yticks(tick_marks, classNames, rotation=90)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm2[i][j]))\n",
    "    plt.plot()\n",
    "    plt.savefig('confusion_matrix2_'+method+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def get_roc_curve(model, X, y):\n",
    "    pred_proba = model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y, pred_proba)\n",
    "    return fpr, tpr\n",
    "def result_from_predicitions(prediction_array,title):\n",
    "    \n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*30)\n",
    "    df1=pd.DataFrame({'Actual':prediction_array[0], 'Predicted':prediction_array[1]})\n",
    "    print(df1)\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    \n",
    "    df2 = pd.DataFrame({'Accuracy:': np.round(metrics.accuracy_score(prediction_array[0],prediction_array[1]),4),\\\n",
    "    'Precision:': np.round(metrics.precision_score(prediction_array[0],prediction_array[1]),4),\\\n",
    "    'Recall:': np.round(metrics.recall_score(prediction_array[0],prediction_array[1]),4),\\\n",
    "    'F1 Score:': np.round(metrics.f1_score(prediction_array[0],prediction_array[1]),4)},index=[\"result\"+title])\n",
    "    print(df2)\n",
    "    df2.to_csv(\"result\"+title+\".csv\")\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    print(metrics.classification_report(prediction_array[0],prediction_array[1]))\n",
    "def result_from_predicitions2(prediction_array):\n",
    "    \n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*30)\n",
    "    df1=pd.DataFrame({'Actual':prediction_array[0], 'Predicted':prediction_array[4]})\n",
    "    print(df1)\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(prediction_array[0],prediction_array[4]),4))\n",
    "    print('Precision:', np.round(metrics.precision_score(prediction_array[0],prediction_array[4]),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(prediction_array[0],prediction_array[4]),4))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(prediction_array[0],prediction_array[4]),4))\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    print(metrics.classification_report(prediction_array[0],prediction_array[4]))\n",
    "def plot_learning_curve(estimator, title, X, y, ylim, cv, n_jobs, train_sizes):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "def run_kfold(clf):\n",
    "    kf = KFold(n_splits=10,shuffle=True)\n",
    "    outcomes = []\n",
    "    with open('data/output_data.pickle', 'rb') as handle:\n",
    "        output_data = pickle.load(handle)\n",
    "    with open('data/preproc1.pickle', 'rb') as handle:\n",
    "        input_data = pickle.load(handle)\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(input_data):\n",
    "        fold += 1\n",
    "        X_train, X_test = input_data.values[train_index], input_data.values[test_index]\n",
    "        y_train, y_test = output_data.values[train_index], output_data.values[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        outcomes.append(accuracy)\n",
    "        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n",
    "    mean_outcome = np.mean(outcomes)\n",
    "    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n",
    "#AUCROC for binary class only\n",
    "def plot_roc(result1,result2,title,method):\n",
    "    #courtesy of DATAI https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv\n",
    "    # plot arrows, why? to present accuracy\n",
    "    '''\n",
    "    fig1 = plt.figure(figsize=[20,10])\n",
    "    ax1 = fig1.add_subplot(121,aspect = 'equal')\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    print(result1)\n",
    "    #for i in range(len(result1[3])):\n",
    "    fpr, tpr, _ = roc_curve(result1[0], result1[5])\n",
    "    with open('fpr'+title[0]+method+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(fpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open('tpr'+title[0]+method+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(tpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label=' (AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='red',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC '+title[0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "    ax2 = fig1.add_subplot(122,aspect = 'equal')\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    #for i in range(len(result2[3])):\n",
    "    fpr, tpr, _ = roc_curve(result2[0], result2[5])\n",
    "    with open('fpr'+title[1]+method+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(fpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open('tpr'+title[1]+method+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(tpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label='(AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC ' + title[1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "    '''\n",
    "    figsize=(20, 10)\n",
    "    \n",
    "    ticksize = 14\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "            'axes.labelsize' : labelsize,\n",
    "            'axes.titlesize' : titlesize,\n",
    "            'xtick.labelsize': ticksize,\n",
    "            'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "    fpr_keras1, tpr_keras1, _ = roc_curve(result1[0], result1[3])\n",
    "    fpr_keras2, tpr_keras2, _ = roc_curve(result2[0], result2[3])\n",
    "    auc_keras1 = auc(fpr_keras1, tpr_keras1)\n",
    "    auc_keras2 = auc(fpr_keras2, tpr_keras2)\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras1, tpr_keras1, label= title[0] +'(area = {:.3f})'.format(auc_keras1))\n",
    "    plt.plot(fpr_keras2, tpr_keras2, label= title[1] +'(area = {:.3f})'.format(auc_keras2))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig('roc1_'+method+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def plot_roc2(result1,result2,title,method):\n",
    "    #courtesy of DATAI https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv\n",
    "    # plot arrows, why? to present accuracy\n",
    "    '''\n",
    "    fig1 = plt.figure(figsize=[20,10])\n",
    "    ax1 = fig1.add_subplot(121,aspect = 'equal')\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax1.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    print(result1)\n",
    "    #for i in range(len(result1[3])):\n",
    "    fpr, tpr, _ = roc_curve(result1[0], result1[5])\n",
    "    with open('fpr'+title[0]+method+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(fpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open('tpr'+title[0]+method+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(tpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label=' (AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='red',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC '+title[0])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "    ax2 = fig1.add_subplot(122,aspect = 'equal')\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "        )\n",
    "    ax2.add_patch(\n",
    "        patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "        )\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    #for i in range(len(result2[3])):\n",
    "    fpr, tpr, _ = roc_curve(result2[0], result2[5])\n",
    "    with open('fpr'+title[1]+method+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(fpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open('tpr'+title[1]+method+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(tpr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #plt.plot(fpr, tpr, lw=2, alpha=0.3, label='(AUC = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "            label=r'ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC ' + title[1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "    '''\n",
    "    figsize=(20, 10)\n",
    "    \n",
    "    ticksize = 14\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "            'axes.labelsize' : labelsize,\n",
    "            'axes.titlesize' : titlesize,\n",
    "            'xtick.labelsize': ticksize,\n",
    "            'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "    fpr_keras1, tpr_keras1, _ = roc_curve(result1[0], result1[5])\n",
    "    fpr_keras2, tpr_keras2, _ = roc_curve(result2[0], result2[5])\n",
    "    auc_keras1 = auc(fpr_keras1, tpr_keras1)\n",
    "    auc_keras2 = auc(fpr_keras2, tpr_keras2)\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras1, tpr_keras1, label= title[0] +'(area = {:.3f})'.format(auc_keras1))\n",
    "    plt.plot(fpr_keras2, tpr_keras2, label= title[1] +'(area = {:.3f})'.format(auc_keras2))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig('roc2_'+method+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#perform predictions on classification methods\n",
    "def clf_predictor(input_data,multiclass):\n",
    "    result_logregr = predictor(input_data,None,multiclass)\n",
    "    result_NBG = predictor(input_data,'NBGauss',multiclass)\n",
    "    result_NBM = predictor(input_data,'NBMulti',multiclass)\n",
    "    result_SVM = predictor(input_data,'SVM',multiclass)\n",
    "    result_RF = predictor(input_data,'RF',multiclass)\n",
    "    result_ensemble = predictor(input_data,'ensemble',multiclass)\n",
    "    result_GB = predictor(input_data,'gb',multiclass)\n",
    "    print(\"Evaluation of predictions:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_logregr)\n",
    "    #plots the graph for accuracies over different encoding metods for one particular classification method\n",
    "    print(\"Logistic Regression:\")\n",
    "    print(\"----------------------------------\")\n",
    "    result_logres_acc = [result_logregr[0][2],result_logregr[1][2]]\n",
    "    result_logres_loss = [result_logregr[0][3],result_logregr[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    if multiclass == 'no':\n",
    "        print(\"Naive Bayes (Gaussian):\")\n",
    "        generate_metrics(result_NBG,'NBG')\n",
    "        result_NBG_acc = [result_NBG[0][2],result_NBG[1][2]]\n",
    "        result_NBG_loss = [result_NBG[0][3],result_NBG[1][3]]\n",
    "        print(\"----------------------------------\")\n",
    "    elif multiclass == 'yes':\n",
    "        print(\"Naive Bayes (Multinomial):\")\n",
    "        print(\"----------------------------------\")\n",
    "        generate_metrics(result_NBM,'NBM')\n",
    "        result_NBM_acc = [result_NBM[0][2],result_NBM[1][2]]\n",
    "        result_NBM_loss = [result_NBM[0][3],result_NBM[1][3]]\n",
    "        print(\"----------------------------------\")\n",
    "    print(\"Support Vector Machine:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_SVM,'SVM')\n",
    "    result_SVM_acc = [result_SVM[0][2],result_SVM[1][2]]\n",
    "    result_SVM_loss = [result_SVM[0][3],result_SVM[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Gradient Boosting\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_GB,'gb')\n",
    "    result_GB_acc = [result_GB[0][2],result_GB[1][2]]\n",
    "    result_GB_loss = [result_GB[0][3],result_GB[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Random Forest:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_RF,'RF')\n",
    "    result_RF_acc = [result_RF[0][2],result_RF[1][2]]\n",
    "    result_RF_loss = [result_RF[0][3],result_RF[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Ensemble of Logistic Regression, Naive Bayes (Multinomial) and Random Forest:\")\n",
    "    print(\"----------------------------------\")\n",
    "    generate_metrics(result_ensemble,'ensemble')\n",
    "    result_ensemble_acc = [result_ensemble[0][2],result_ensemble[1][2]]\n",
    "    result_ensemble_loss = [result_ensemble[0][3],result_ensemble[1][3]]\n",
    "    print(\"----------------------------------\")\n",
    "    result_acc = [result_logres_acc[0],result_logres_acc[1], \\\n",
    "    result_NBG_acc[0],result_NBG_acc[1], \\\n",
    "    result_NBM_acc[0],result_NBM_acc[1], \\\n",
    "    result_SVM_acc[0],result_SVM_acc[1], \\\n",
    "    result_RF_acc[0],result_RF_acc[1], \\\n",
    "    result_ensemble_acc[0],result_ensemble_acc[1],result_GB_acc[0],result_GB_acc[1]]\n",
    "\n",
    "    result_loss = [result_logres_loss[0],result_logres_loss[1], \\\n",
    "    result_NBG_loss[0],result_NBG_loss[1], \\\n",
    "    result_NBM_loss[0],result_NBM_loss[1], \\\n",
    "    result_SVM_loss[0],result_SVM_loss[1], \\\n",
    "    result_RF_loss[0],result_RF_loss[1], \\\n",
    "    result_ensemble_loss[0],result_ensemble_loss[1],result_GB_loss[0],result_GB_loss[1]]\n",
    "\n",
    "    proba_data = [result_logregr[0][4],result_logregr[1][4],result_NBG[0][4],result_NBG[1][4],\\\n",
    "        result_NBM[0][4],result_NBM[1][4],result_SVM[0][4],result_SVM[1][4],\\\n",
    "        result_GB[0][4],result_GB[1][4],result_RF[0][4],result_RF[1][4],\\\n",
    "        result_ensemble[0][4],result_ensemble[1][4]]\n",
    "    test_data = [result_logregr[0][1],result_logregr[1][1],result_NBG[0][1],result_NBG[1][1],\\\n",
    "        result_NBM[0][1],result_NBM[1][1],result_SVM[0][1],result_SVM[1][1],\\\n",
    "        result_GB[0][1],result_GB[1][1],result_RF[0][1],result_RF[1][1],\\\n",
    "        result_ensemble[0][1],result_ensemble[1][1]]\n",
    "  \n",
    "    result1 = [result_acc, result_loss]\n",
    "    result2 = [proba_data,test_data]\n",
    "    result=[result1,result2]\n",
    "    return result\n",
    "#perform word embeddings inputs: dataframe input data and output data; output: embedded data such as X train and test and y train and test\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('swedish')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        \n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "#category is binary by default\n",
    "def word_embeddings(input_data, output_data,ANN,dense,el,category=None):\n",
    "    \n",
    "    #Don't use stop words removal for deep learning\n",
    "    #input_data = input_data.apply(remove_stopwords)\n",
    "\n",
    "\n",
    "    # Summarize number of words\n",
    "    \n",
    "    data = feature_engineering(input_data, output_data)\n",
    "    '''\n",
    "    global model2\n",
    "    resmod2 = [None] * input_data.shape[0]\n",
    "    #print(np.array(input_data)[0])\n",
    "    for i in range(input_data.shape[0]):\n",
    "        resmod = model2.predict(np.array(input_data)[i])\n",
    "        print(resmod[0])\n",
    "        lst = list(resmod)\n",
    "        if lst[0] == \"('label_1',)\":\n",
    "            lst[0] = 1\n",
    "        else:\n",
    "            lst[0] = 0\n",
    "        \n",
    "        resmod2.append(tuple(lst))\n",
    "    print(resmod2)\n",
    "    resmod = [value for x in resmod2 for value in x and value is not None]\n",
    "    resmod = np.array(resmod2)\n",
    "    '''\n",
    "\n",
    "\n",
    "    print(\"Train set has total {0} entries with {1:.2f}% 0, {2:.2f}% 1\".format(len(data[0]),\n",
    "                                                                             (len(data[0][data[2] == 0]) / (len(data[0])*1.))*100,\n",
    "                                                                        (len(data[0][data[2] == 1]) / (len(data[0])*1.))*100))\n",
    "    print(\"Test set has total {0} entries with {1:.2f}% 0, {2:.2f}% 1\".format(len(data[1]),\n",
    "                                                                             (len(data[1][data[3] == 0]) / (len(data[1])*1.))*100,\n",
    "                                                                            (len(data[1][data[3] == 1]) / (len(data[1])*1.))*100))\n",
    "    \n",
    "    data_out = we_output_data_transform(data[2],data[3])\n",
    "    '''\n",
    "    if category == None:\n",
    "        data = feature_engineering(input_data, output_data)\n",
    "        data_out = we_output_data_transform(data[2],data[3])\n",
    "    else:\n",
    "        with open('df.pickle', 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "        count = len(df.index)\n",
    "        data = feature_engineering(input_data, output_data)\n",
    "        dataset,data_out = we_output_data_transform(data[2],data[3],'multi')\n",
    "        data_in1 = multivectorizer(dataset)\n",
    "        data_in2 = feature_engineering(data_in1[0], data_out[:count])\n",
    "        #validation data\n",
    "        data2 = feature_engineering(data_in2[0], data_out[0])\n",
    "    '''\n",
    "    \n",
    "    #index 0 = X_train\n",
    "    #index 1 = X_test\n",
    "    #index 2 = y_train\n",
    "    #index 3 = y_test\n",
    "    assert data[0].shape[0] == data[2].shape[0]\n",
    "    assert data[1].shape[0] == data[3].shape[0]\n",
    "    data_in1 = tokenizer(input_data,data[0], data[1])\n",
    "    X = np.concatenate((data_in1[0], data_in1[1]), axis=0)\n",
    "    y = np.concatenate((data_out[0], data_out[1]), axis=0)\n",
    "    print(\"Number of words: \")\n",
    "    print(len(np.unique(np.hstack(X))))\n",
    "    print(\"Review length: \")\n",
    "    result = [len(x) for x in X]\n",
    "    print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "    # plot review length\n",
    "    #plt.boxplot(result)\n",
    "    #plt.show()\n",
    "    #print(data_in1[2])\n",
    "    data_in2 = padding(data_in1[0], data_in1[1],input_data)\n",
    "    global MAX_SEQUENCE_LENGTH\n",
    "    MAX_SEQUENCE_LENGTH = data_in2[0].shape[1]\n",
    "    '''\n",
    "    data_in21 = vectorize_sequences(input_data)\n",
    "    '''\n",
    "    \n",
    "    #create validation data \n",
    "    data2 = feature_engineering(data_in2[0], data_out[0])\n",
    "    #data2[0] = X_val\n",
    "    #data2[2] = y_val\n",
    "    assert data2[1].shape[0] == data2[3].shape[0]\n",
    "    assert data2[0].shape[0] == data2[2].shape[0]\n",
    "    #fasttext (word_to_vec_map, word_to_index, index_to_words, vocab_size, dim)\n",
    "    #tip: try to swap the sv.vec file with cc.sv.300.vec\n",
    "    #load fasttext data into cnn1\n",
    "    #save\n",
    "    with open('data/data_in2.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_in2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/data_out.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_out, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    '''\n",
    "    with open('data_in.pickle', 'rb') as handle:\n",
    "        data_in = pickle.load(handle)\n",
    "    with open('data_out.pickle', 'rb') as handle:\n",
    "        data_out = pickle.load(handle)\n",
    "    '''\n",
    "   \n",
    "    corpus = load_vectors2('./data/fasttext/sv.vec')\n",
    "    #data_train = sentences_to_indices(data[0],corpus[1],len(data_in1[2]))\n",
    "    #data_test = sentences_to_indices(data[1],corpus[1],len(data_in1[2]))\n",
    "    '''\n",
    "    wiki_news = './cc.sv.300.vec'\n",
    "    embed_fasttext = load_embed(wiki_news)\n",
    "    #step1 build vocab\n",
    "    vocab = build_vocab(input_data)\n",
    "    #vocab is your embedding matrix\n",
    "    #step2 check coverage\n",
    "    print(\"FastText : \")\n",
    "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "    print(oov_fasttext[:18])\n",
    "    '''\n",
    "    vocab = None\n",
    "    #change corpus[1] to data_in[2]\n",
    "    embedding_layer1 = pretrained_embedding_layer(corpus[0], corpus[1],vocab)\n",
    "    embedding_layer1=1\n",
    "    print(corpus[4])\n",
    "    \n",
    "    embedding_layer0=load_vectors_word2vec('./data/word2vec/sv.bin',data_in1[2])\n",
    "    \n",
    "    embedding_layer = [embedding_layer0,embedding_layer1]\n",
    "    params = {\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'output_dim':[32,64,100,128,300]\n",
    "     'batch_size1' : [64, 128, 256, 512, 1024],\n",
    "     'epochs': [3,4,5],\n",
    "     'dropout': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "     'kernel_initializer': ['uniform','normal'],\n",
    "     'optimizer': [Adam,Adadelta],\n",
    "     'activation':[tanh,relu, elu]}\n",
    "    #second preprocessing method\n",
    "    #change data_in2[0].shape[1] with (MAX_LEN,)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(date_time(1))\n",
    "    model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_in2[0],data_out[0],data2[1],data2[3],1,params)\n",
    "    #or\n",
    "    #model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_train,data_out[0],None,None,dense)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print(\"\\nElapsed Time: \" + elapsed_time)\n",
    "    print(\"Completed Model Trainning\", date_time(1))\n",
    "    '''\n",
    "    with open('model'+ANN+'.pickle', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    '''\n",
    "    #data_in2[0] = X_train\n",
    "    newmodel = ta.Scan(data_in2[0], data_out[0], params=params,\n",
    "            model=model,\n",
    "            experiment_no='1', print_params=True)\n",
    "    data2 = [data_in2[0],data_in2[1],data_out[0],data_out[1]]\n",
    "    return [data_in2, data2, data,newmodel]\n",
    "def vectorize_sequences(sequences, dimension=4900):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "def multivectorization(concated):\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(concated['freetext'].values)\n",
    "    sequences = tokenizer.texts_to_sequences(concated['freetext'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "    result = [X,word_index]\n",
    "    return result\n",
    "def we_evaluation(model,model1,data1,data2,data3,data4,ANN1,ANN2 ,datax,datay):\n",
    "    '''\n",
    "    preds1 = model.predict(data1[1],batch_size=13)\n",
    "    preds2 = model1.predict(data3[1],batch_size=13)\n",
    "    \n",
    "    #binary classification\n",
    "    preds1=[1 * (x[0]>=0.5) for x in preds1]\n",
    "    \n",
    "    #categorical\n",
    "    preds1 = np.argmax(preds1, axis=-1)\n",
    "    \n",
    "    #binary classification\n",
    "    preds2=[1 * (x[0]>=0.5) for x in preds2]\n",
    "    \n",
    "    #categorical\n",
    "    preds2 = np.argmax(preds2, axis=-1)\n",
    "    '''\n",
    "    best_model1 = ta.Predict(model)\n",
    "    best_model2 = ta.Predict(model1)\n",
    "    evaluate_model1 = ta.Evaluate(model)\n",
    "    evaluate_model2 = ta.Evaluate(model1)\n",
    "    #predicting using X_test\n",
    "    preds1 = best_model1.predict(data1[1])\n",
    "    preds2 = best_model2.predict(data3[1])\n",
    "    preds3 = best_model1.predict(data1[1]).ravel()\n",
    "    preds4 = best_model2.predict(data1[1]).ravel()\n",
    "    \n",
    "    #crossvalidation using 10-fold\n",
    "    eval_model1 = best_model1.evaluate(data1[1], data2[3],average='micro')\n",
    "    eval_model2 = best_model2.evaluate(data3[1], data4[3],average='micro')\n",
    "    \n",
    "    print(\"evaluation of model1: \")\n",
    "    print(evaluate_model1)\n",
    "    print(\"evaluation of model2: \")\n",
    "    print(evaluate_model2)\n",
    "    \n",
    "    #preds1_cv = best_model1.evaluate(data1[0], data2[2], folds=10, average='macro')\n",
    "    #preds1_cv = best_model1.evaluate(data1[0], data2[2], folds=10, average='macro')\n",
    "    \n",
    "    with open('data/preds1.pickle', 'wb') as handle:\n",
    "        pickle.dump(preds1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/preds2.pickle', 'wb') as handle:\n",
    "        pickle.dump(preds2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/test_data1.pickle', 'wb') as handle:\n",
    "        pickle.dump(datax[3], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/test_data2.pickle', 'wb') as handle:\n",
    "        pickle.dump(datay[3], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    test1 = ANN1\n",
    "    test2 = ANN2\n",
    "    title = [test1,test2]\n",
    "    #keras evaluation\n",
    "    #evaluating using X_test and y_test\n",
    "    score = model.evaluate(data1[1], data2[3], verbose=1)\n",
    "    print(\"Model Performance of model 1: \"+ test1 +\" (Test):\")\n",
    "    df_score1=pd.DataFrame.from_records([{'Accuracy':score[1],'Precision':score[2],'Recall':score[3],'F1_score':score[4]}])\n",
    "    print(df_score1)\n",
    "    score = model1.evaluate(data3[1], data4[3], verbose=1)\n",
    "    print(\"Model Performance of model 2: \"+ test2 +\" (Test):\")\n",
    "    df_score2=pd.DataFrame.from_records([{'Accuracy':score[1],'Precision':score[2],'Recall':score[3],'F1_score':score[4]}])\n",
    "    print(df_score2)\n",
    "    \n",
    "    with open('data/preds1.pickle', 'rb') as handle:\n",
    "        preds1 = pickle.load(handle)\n",
    "    with open('data/preds2.pickle', 'rb') as handle:\n",
    "        preds2 = pickle.load(handle)\n",
    "    with open('data/test_data1.pickle', 'rb') as handle:\n",
    "        test_data1 = pickle.load(handle)\n",
    "    with open('data/test_data2.pickle', 'rb') as handle:\n",
    "        test_data2 = pickle.load(handle)\n",
    "    #SKLearn Evaluation\n",
    "    target_class = ['class_0','class_1']\n",
    "    labels = [0,1]\n",
    "    print(\"Results from prediction:\")\n",
    "    print('-'*200)\n",
    "    df1=pd.DataFrame({'Actual':test_data1, 'Predicted':preds1})\n",
    "    print(df1)\n",
    "    print('-'*200)\n",
    "    df2=pd.DataFrame({'Actual':test_data2, 'Predicted':preds2})\n",
    "    print(df2)\n",
    "    df2 = pd.DataFrame({'Accuracy:': np.round(metrics.accuracy_score(test_data2,preds1),4),\\\n",
    "    'Precision:': np.round(metrics.precision_score(test_data2,preds1),4),\\\n",
    "    'Recall:': np.round(metrics.recall_score(test_data2,preds1),4),\\\n",
    "    'F1 Score:': np.round(metrics.f1_score(test_data2,preds1),4)},index=[\"result\"+title[0]])\n",
    "    print(df2)\n",
    "    df2.to_csv(\"result\"+title[0]+\".csv\")\n",
    "    df2 = pd.DataFrame({'Accuracy:': np.round(metrics.accuracy_score(test_data2,preds2),4),\\\n",
    "    'Precision:': np.round(metrics.precision_score(test_data2,preds2),4),\\\n",
    "    'Recall:': np.round(metrics.recall_score(test_data2,preds2),4),\\\n",
    "    'F1 Score:': np.round(metrics.f1_score(test_data2,preds2),4)},index=[\"result\"+title[1]])\n",
    "    print(df2)\n",
    "    df2.to_csv(\"result\"+title[1]+\".csv\")\n",
    "    print('-'*200)\n",
    "    print(metrics.classification_report(test_data1,preds1,labels,target_class))\n",
    "    print('-'*200)\n",
    "    print(metrics.classification_report(test_data2,preds2,labels,target_class))\n",
    "    array1 = [test_data1,preds1]\n",
    "    array2 = [test_data2,preds2]\n",
    "    method = test1+'_'+test2\n",
    "    method1 = [test1,test2]\n",
    "    plot_classification_report(array1,array2,title,method)\n",
    "    print('-'*200)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*200)\n",
    "    cm1 = metrics.confusion_matrix(y_true=test_data1, y_pred=preds1)\n",
    "    print(cm1)\n",
    "    cm2 = metrics.confusion_matrix(y_true=test_data2, y_pred=preds2)\n",
    "    print(cm2)\n",
    "    plot_cm(cm1,cm2,method,method1)\n",
    "\n",
    "    \n",
    "    \n",
    "    #works only in binary_crossentropy\n",
    "    \n",
    "    del model\n",
    "    del model1\n",
    "    gc.collect()\n",
    "    fpr_keras1, tpr_keras1, _ = roc_curve(test_data1, preds3)\n",
    "    fpr_keras2, tpr_keras2, _ = roc_curve(test_data2, preds4)\n",
    "    auc_keras1 = auc(fpr_keras1, tpr_keras1)\n",
    "    auc_keras2 = auc(fpr_keras2, tpr_keras2)\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras1, tpr_keras1, label= test1 +'(area = {:.3f})'.format(auc_keras1))\n",
    "    plt.plot(fpr_keras2, tpr_keras2, label= test2 +'(area = {:.3f})'.format(auc_keras2))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    df1.to_csv('prediction1.csv', encoding='utf-8', index=True)\n",
    "    df2.to_csv('prediction2.csv', encoding='utf-8', index=True)\n",
    "    \n",
    "    del test_data1\n",
    "    del test_data2\n",
    "    del data1\n",
    "    del data2\n",
    "    del data3\n",
    "    del data4\n",
    "    del datax\n",
    "    del datay\n",
    "def sentences_to_indices(X, word_to_index, maxLen):\n",
    "    m = X.shape[0] \n",
    "    X = np.array(X)\n",
    "    X_indices = np.zeros((m, maxLen))\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().strip().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index:\n",
    "                w = \"person\"  \n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices\n",
    "def plot_function(track):\n",
    "    plt.subplot(221)\n",
    "    plt.plot(track.history['acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(track.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.show()\n",
    "def plot_classification_report(array1, array2,title,method, ax=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(211)\n",
    "    \n",
    "    plt.title('Classification Report '+title[0])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array1[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array1[0], array1[1])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    \n",
    "    plt.title('Classification Report '+title[1])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array2[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array2[0], array2[1])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "    \n",
    "    \n",
    "    plt.savefig('classification_report_'+ method +'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def plot_classification_report2(array1, array2,title,method, ax=None):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(211)\n",
    "    \n",
    "    plt.title('Classification Report '+title[0])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array1[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array1[0], array1[4])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    \n",
    "    plt.title('Classification Report '+title[1])\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = list(np.unique(array2[0]))\n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(array2[0], array2[4])).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax)\n",
    "    \n",
    "    \n",
    "    plt.savefig('classification_report2_'+ method +'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#tokenizes the words\n",
    "def tokenizer(input_data,train_data, test_data):\n",
    "    #from [7]\n",
    "    seq_lengths = input_data.apply(lambda x: len(x.split(' ')))\n",
    "    print(seq_lengths.describe())\n",
    "    \n",
    "    tk = Tokenizer(num_words=NB_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \")\n",
    "    tk.fit_on_texts(input_data)\n",
    "    trained_seq = tk.texts_to_sequences(train_data.values)\n",
    "    test_seq = tk.texts_to_sequences(test_data.values)\n",
    "    word_index = tk.word_index\n",
    "    result = [trained_seq, test_seq, word_index]\n",
    "    return result\n",
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n",
    "#test function from [7] to make sure that the sequences generated from the tokenizer function are of equal length\n",
    "def test_sequence(train_data):\n",
    "    seq_lengths = train_data.apply(lambda x: len(x.split(' ')))\n",
    "    print(\"The sequences generated are:\")\n",
    "    seq_lengths.describe()\n",
    "    print(\"----------------\")\n",
    "def date_time(x):\n",
    "    if x==1:\n",
    "        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==2:    \n",
    "        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==3:  \n",
    "        return 'Date now: %s' % datetime.datetime.now()\n",
    "    if x==4:  \n",
    "        return 'Date today: %s' % datetime.date.today() \n",
    "def plot_performance(title,history=None, figure_directory=None, ylim_pad=[0, 0]):\n",
    "    xlabel = 'Epoch'\n",
    "    legends = ['Training', 'Validation']\n",
    "    if title is None:\n",
    "        title = 'GRU2'\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.suptitle(title,fontsize=17)\n",
    "    y1 = history.history['f1_score1']\n",
    "    y2 = history.history['val_f1_score1']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[0]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[0]\n",
    "\n",
    "    plt.subplot(221)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model F1_score\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('F1_score', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['loss']\n",
    "    y2 = history.history['val_loss']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(222)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Loss', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['precision']\n",
    "    y2 = history.history['val_precision']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(223)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Precision\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Precision', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['recall']\n",
    "    y2 = history.history['val_recall']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(224)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Recall\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Recall', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "    if figure_directory:\n",
    "        plt.savefig(figure_directory+\"/history\")\n",
    "\n",
    "    plt.show()\n",
    "#in [7] padding is used to fill out null values\n",
    "def padding(trained_seq, test_seq,input):\n",
    "    trained_seq_trunc = pad_sequences(trained_seq,maxlen=MAX_LEN)\n",
    "    test_seq_trunc = pad_sequences(test_seq,maxlen=MAX_LEN)\n",
    "    print(trained_seq_trunc.shape)\n",
    "    print(test_seq_trunc.shape)\n",
    "    result = [trained_seq_trunc, test_seq_trunc]\n",
    "    return result\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == './cc.sv.300.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "def we_output_data_transform(y_train,y_test,encoding='binary',category=None):\n",
    "    if encoding == 'multi':\n",
    "        '''\n",
    "        From Peter Nagy, Kaggle, https://www.kaggle.com/ngyptr/multi-class-classification-with-lstm\n",
    "        '''\n",
    "        with open('data/df.pickle', 'rb') as handle:\n",
    "            df = pickle.load(handle)\n",
    "        df1 = df['freetext']\n",
    "        df2 = df['pout']\n",
    "        frames = [df1,df2]\n",
    "        data = pd.concat(frames, axis=1, sort=False)\n",
    "        num_of_categories = df.count+1\n",
    "        shuffled = data.reindex(np.random.permutation(data.index))\n",
    "        prio1A = shuffled[shuffled['pout'] == '1A'][:num_of_categories]\n",
    "        prio1B = shuffled[shuffled['pout'] == '1B'][:num_of_categories]\n",
    "        prio2A = shuffled[shuffled['pout'] == '2A'][:num_of_categories]\n",
    "        prio2B = shuffled[shuffled['pout'] == '2B'][:num_of_categories]\n",
    "        Referal = shuffled[shuffled['pout'] == 'Referal'][:num_of_categories]\n",
    "        concated = pd.concat([prio1A,prio1B,prio2A,prio2B,Referal], ignore_index=True)\n",
    "        concated = concated.reindex(np.random.permutation(concated.index))\n",
    "        concated['LABEL'] = 0\n",
    "        concated.loc[concated['pout'] == '1A', 'LABEL'] = 0\n",
    "        concated.loc[concated['pout'] == '1B', 'LABEL'] = 1\n",
    "        concated.loc[concated['pout'] == '2A', 'LABEL'] = 2\n",
    "        concated.loc[concated['pout'] == '2B', 'LABEL'] = 3\n",
    "        concated.loc[concated['pout'] == 'Referal', 'LABEL'] = 4\n",
    "        labels = to_categorical(concated['LABEL'], num_classes=5)\n",
    "        if 'pout' in concated.keys():\n",
    "            concated.drop(['pout'], axis=1)\n",
    "        return concated,labels\n",
    "    else:\n",
    "        #le = LabelEncoder()\n",
    "        #y_train_le = le.fit_transform(y_train.values)\n",
    "        #y_test_le = le.fit_transform(y_test.values)\n",
    "        \n",
    "        #binary_crossentropy\n",
    "        y_train = np.array(y_train.values)\n",
    "        y_test = np.array(y_test.values)\n",
    "        '''\n",
    "        \n",
    "        #categorical_crossentropy\n",
    "        \n",
    "        y_train = np.array(to_categorical(y_train.values).astype('float32'))\n",
    "        y_test = np.array(to_categorical(y_test.values).astype('float32'))\n",
    "        '''\n",
    "        \n",
    "    result = [y_train,y_test]\n",
    "    return result\n",
    "#embeddings layer\n",
    "def embeddings_layer(X_train_emb, X_valid_emb,y_train_emb,y_valid_emb,dense):\n",
    "    emb_model = models.Sequential()\n",
    "    emb_model.add(layers.Embedding(NB_WORDS, DIM, input_length=MAX_LEN))\n",
    "    emb_model.add(layers.Flatten())\n",
    "    emb_model.add(layers.Dense(dense, activation='softmax'))\n",
    "    emb_model.summary()\n",
    "    emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)\n",
    "    result = emb_history\n",
    "    return result\n",
    "#from [7]\n",
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=1)\n",
    "    \n",
    "    return history\n",
    "#load vec file from fasttext, from [8]\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    vocab_size, dim = map(int, fin.readline().split())\n",
    "    word_to_vec_map = {}\n",
    "    words = set()\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        words.add(tokens[0])\n",
    "        word_to_vec_map[tokens[0]] = np.array(tokens[1:], dtype=np.float64)\n",
    "    i = 1\n",
    "    words_to_index = {}\n",
    "    index_to_words = {}\n",
    "    for w in sorted(words):\n",
    "        words_to_index[w] = i\n",
    "        index_to_words[i] = w\n",
    "        i = i + 1\n",
    "    return word_to_vec_map, words_to_index, index_to_words, vocab_size, dim\n",
    "#caller function for load_vectors .vec file\n",
    "def load_vectors2(fname):\n",
    "    word_to_vec_map, words_to_index, index_to_words, vocab_size, dim = load_vectors(fname)\n",
    "    result = [word_to_vec_map, words_to_index, index_to_words, vocab_size, dim]\n",
    "    return result\n",
    "#from[8]\n",
    "#for Word2Vec input word2vec .bin file and word_index = tokenizer.word_index from tokenizer\n",
    "def load_vectors_word2vec(fname,word_index):\n",
    "    word_vectors = KeyedVectors.load(fname)\n",
    "    \n",
    "    vocabulary_size = min(MAX_NB_WORDS, len(word_index))+1\n",
    "    embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i>=NB_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "    del(word_vectors)\n",
    "    embedding_layer = Embedding(NB_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "    embedding_layer.build(MAX_LEN)\n",
    "    return embedding_layer\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")\n",
    "#[8]\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index,embeddings_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    with open('data/words.pickle', 'wb') as handle:\n",
    "        pickle.dump(vocab_len, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    emb_dim = 300\n",
    "    '''\n",
    "    #or\n",
    "    emb_dim = 300\n",
    "    '''\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim,embeddings_initializer=Constant(emb_matrix),\n",
    "    input_length = MAX_LEN,trainable=False) \n",
    "\n",
    "    \n",
    "    return embedding_layer\n",
    "#bilstm\n",
    "def bilstm(X_train,y_train,X_val,y_val,embedding_layer1,dense,params):\n",
    "    model = Sequential()\n",
    "    #try and comment this out and test again\n",
    "    embedding_layer1=Embedding(28352, 64, input_length=MAX_LEN)\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Bidirectional(LSTM(units=100,params['activation'])))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(dense,activation='sigmoid',kernel_initializer=params['kernel_initializer']))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.001,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score1])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1,validation_data=(X_val, y_val),callbacks=[live()])\n",
    "    return history,model\n",
    "def cnn2(X_train,y_train,X_val,y_val,embedding_layer1,dense,params):\n",
    "    \n",
    "    model = Sequential()\n",
    "    #try and comment this out and test again\n",
    "    embedding_layer1=Embedding(28352, 64, input_length=MAX_LEN)\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(units=100,params['activation']))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(dense,activation='sigmoid',kernel_initializer=params['kernel_initializer']))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    adam = Adam(lr=0.001,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score1])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1,validation_data=(X_val, y_val),callbacks=[live()])\n",
    "    return history,model\n",
    "def cnn1(X_train,y_train,X_val,y_val,embedding_layer1,dense,params):\n",
    "    \n",
    "    model = Sequential()\n",
    "    #try and comment this out and test again\n",
    "    embedding_layer1=Embedding(28352, 64, input_length=MAX_LEN)\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Bidirectional(LSTM(units=100)))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(dense,activation='sigmoid',kernel_initializer=params['kernel_initializer']))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.001,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score1])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1,validation_data=(X_val, y_val),callbacks=[live()])\n",
    "    return history,model\n",
    "def gru_model(X_train,y_train,X_val,y_val,embedding_layer1,dense,params):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    #try and comment this out and test again\n",
    "    embedding_layer1=Embedding(28352, 64, input_length=MAX_LEN)\n",
    "    model.add(embedding_layer1)\n",
    "    #model.add(SpatialDropout1D(0.1))\n",
    "    model.add(GRU(units=100))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    #model.add(Dense(dense,activation='sigmoid',kernel_regularizer=regularizers.l2(1e-3),activity_regularizer=regularizers.l1(1e-4)))\n",
    "    model.add(Dense(dense,activation='sigmoid',kernel_initializer=params['kernel_initializer']))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.002)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy',precision,recall,f1_score1])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1,validation_data=(X_val, y_val),callbacks=[live()])\n",
    "    return history,model\n",
    "def lstm_model(X_train,y_train,X_val,y_val,embedding_layer1,dense,params):\n",
    "    \n",
    "    model = Sequential()\n",
    "    #try and comment this out and test again\n",
    "    embedding_layer1=Embedding(28352, 64, input_length=MAX_LEN)\n",
    "    model.add(embedding_layer1)\n",
    "    #model.add(SpatialDropout1D(0.1))\n",
    "    model.add(LSTM(units=100))\n",
    "    \n",
    "    #model.add(Dense(234,activation='elu',kernel_regularizer=regularizers.l2(1e-9),activity_regularizer=regularizers.l1(1e-9),bias_regularizer=regularizers.l2(0.01), kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(dense,activation='sigmoid',kernel_initializer=params['kernel_initializer']))\n",
    "    #,kernel_regularizer=regularizers.l2(1e-3),activity_regularizer=regularizers.l1(1e-4)))\n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.002)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score1])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1,validation_data=(X_val, y_val),callbacks=[live()])\n",
    "    return history,model\n",
    "    #from https://www.kaggle.com/sakarilukkarinen/embedding-lstm-gru-and-conv1d/versions\n",
    "def gru_model2(X_train,y_train,X_val,y_val,embedding_layer1,dense,params):\n",
    "\n",
    "    model = Sequential()\n",
    "    #try and comment this out and test again\n",
    "    embedding_layer1=Embedding(28352, 64, input_length=MAX_LEN)\n",
    "    model.add(embedding_layer1)\n",
    "    model.add(Bidirectional(GRU(units=100)))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(dense,activation='sigmoid'))    \n",
    "    #sgd = SGD(lr=0.004, decay=1e-7, momentum=0.3, nesterov=True)\n",
    "    #adam = Adam(lr=0.009,epsilon=0.09,amsgrad=True,decay=0)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy',precision,recall,f1_score1])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1,validation_data=(X_val, y_val),callbacks=[live()])\n",
    "    return history,model\n",
    "def predict_model(input_shape,embedding_layer,model_type,X_train,y_train,X_val,y_val,dense,params):\n",
    "    callbacks = [EarlyStopping(monitor='val_loss',patience=2)]\n",
    "    #you can also use rmsprop as optimizer\n",
    "    #adam = Adam(lr=1e-3)\n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    #loss = 'categorical_crossentropy'\n",
    "    \n",
    "    if model_type == 'cnn1':\n",
    "        \n",
    "        history,model = cnn1(X_train,y_train,X_val,y_val,embedding_layer,dense,params)\n",
    "        plot_performance(model_type,history)\n",
    "    elif model_type == 'cnn2':\n",
    "        \n",
    "        history,model = cnn2(X_train,y_train,X_val,y_val,embedding_layer,dense,params) \n",
    "        plot_performance(model_type,history)\n",
    "    elif model_type == 'lstm':\n",
    "        \n",
    "        history,model = lstm_model(X_train,y_train,X_val,y_val,embedding_layer,dense,params)  \n",
    "        plot_performance(model_type,history)\n",
    "    elif model_type == 'bilstm':\n",
    "        \n",
    "        history,model = bilstm(X_train,y_train,X_val,y_val,embedding_layer,dense,params) \n",
    "        plot_performance(model_type,history)\n",
    "    elif model_type == 'gru':\n",
    "       \n",
    "        history,model = gru_model(X_train,y_train,X_val,y_val,embedding_layer,dense,params) \n",
    "        plot_performance(model_type,history)\n",
    "    else:\n",
    "        \n",
    "        history,model = gru_model2(X_train,y_train,X_val,y_val,embedding_layer,dense,params)\n",
    "        plot_performance(model_type,history)\n",
    "    #print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "    #model = None\n",
    "    return model\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "def plot_model_performace(result):\n",
    "    \n",
    "    sns.set_style(\"ticks\")\n",
    "    figsize=(22, 6)\n",
    "\n",
    "    ticksize = 12\n",
    "    titlesize = ticksize + 8\n",
    "    labelsize = ticksize + 5\n",
    "\n",
    "    xlabel = \"Model\"\n",
    "    ylabel = \"Score\"\n",
    "\n",
    "    title = \"Model Performance\"\n",
    "\n",
    "    params = {'figure.figsize' : figsize,\n",
    "              'axes.labelsize' : labelsize,\n",
    "              'axes.titlesize' : titlesize,\n",
    "              'xtick.labelsize': ticksize,\n",
    "              'ytick.labelsize': ticksize}\n",
    "\n",
    "    plt.rcParams.update(params)\n",
    "\n",
    "    col1 = \"model\"\n",
    "    col2 = \"score\"\n",
    "    sns.barplot(x=col1, y=col2, data=result)\n",
    "\n",
    "    plt.title(title.title())\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid()\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split), ignore_index=True)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "#to call the function above, do this: parallelize_dataframe(df, cleaning), then pickle\n",
    "#from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras, visited 26th may\n",
    "def f1_score1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "'''\n",
    "[1] https://medium.com/deep-learning-turkey/text-processing-1-old-fashioned-methods-bag-of-words-and-tfxidf-b2340cc7ad4b, Medium, Deniz Kilinc visited 6th of April 2019\n",
    "[2] https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm, from ReiiNakano , Kaggle, visited 5th of April 2019\n",
    "[3] https://github.com/codebasics/py/tree/master/ML, github, Codebasics from dhavalsays, visited 6th of April 2019\n",
    "[4] from scikit-learn.org (base code), visited 4th of April 2019\n",
    "[5] Python One Hot Encoding with SciKit Learn, InsightBot, http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn, visited 6th April 2019 \n",
    "[6] Kaggle, Sentiment Analysis : CountVectorizer & TF-IDF, Divyojyoti Sinha, https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\n",
    "[7] Kaggle, Bert Carremans, Using Word Embeddings for Sentiment Analysis, https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis, visited april 11th 2019\n",
    "[8] Sentiment Analysis with pretrained Word2Vec, Varun Sharma, Kaggle, https://www.kaggle.com/varunsharmaml/sentiment-analysis-with-pretrained-word2vec, visited 12th of april 2019\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame(columns=[\"Method\",\"Accuracy\"])\n",
    "df = pd.read_csv(\"190203_data_exjobb.csv\", encoding='ISO-8859-1')\n",
    "df = transformAge(df)\n",
    "df = transformLCD(df)\n",
    "\n",
    "\n",
    "input_data = df.freetext.astype(str)\n",
    "input_data1 = df.iloc[:,1:2].freetext.astype(str)\n",
    "output_data = df.hosp_admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 25516 entries with 54.27% 0, 45.73% 1\n",
      "Test set has total 2836 entries with 54.23% 0, 45.77% 1\n",
      "count    28352.000000\n",
      "mean        23.270316\n",
      "std         11.727775\n",
      "min          1.000000\n",
      "25%         15.000000\n",
      "50%         21.000000\n",
      "75%         30.000000\n",
      "max        134.000000\n",
      "Name: freetext, dtype: float64\n",
      "Number of words: \n",
      "22588\n",
      "Review length: \n",
      "Mean 23.06 words (11.601481)\n",
      "(25516, 134)\n",
      "(2836, 134)\n",
      "300\n",
      "WARNING:tensorflow:From C:\\Users\\spd001\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Timestamp: 2019-06-11 16:38:19\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 134, 64)           1814528   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,880,629\n",
      "Trainable params: 1,880,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\spd001\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25516 samples, validate on 2836 samples\n",
      "Epoch 1/3\n",
      "25516/25516 [==============================] - 85s 3ms/step - loss: 0.6316 - acc: 0.6436 - precision: 0.6197 - recall: 0.5160 - f1_score1: 0.5449 - val_loss: 0.5972 - val_acc: 0.6886 - val_precision: 0.6833 - val_recall: 0.5909 - val_f1_score1: 0.6287- f1_scor - ETA: 35s - loss: 0.6474 - acc: 0.6 - ETA: 6s - loss: 0.6330 - acc: 0.6\n",
      "Epoch 2/3\n",
      "25516/25516 [==============================] - 83s 3ms/step - loss: 0.5335 - acc: 0.7388 - precision: 0.7255 - recall: 0.6947 - f1_score1: 0.7044 - val_loss: 0.6161 - val_acc: 0.6685 - val_precision: 0.6404 - val_recall: 0.6138 - val_f1_score1: 0.6231\n",
      "Epoch 3/3\n",
      " 8704/25516 [=========>....................] - ETA: 52s - loss: 0.3966 - acc: 0.8192 - precision: 0.8071 - recall: 0.7886 - f1_score1: 0.7948"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-45b86a5b1497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreetext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moutput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhosp_admit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mANN1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtest1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mANN2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mwe_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mANN1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mANN2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4fea8c9b5cee>\u001b[0m in \u001b[0;36mword_embeddings\u001b[1;34m(input_data, output_data, ANN, dense, el, category)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mANN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_in2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_in2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;31m#or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[1;31m#model = predict_model(MAX_LEN,embedding_layer[el],ANN,data_train,data_out[0],None,None,dense)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4fea8c9b5cee>\u001b[0m in \u001b[0;36mpredict_model\u001b[1;34m(input_shape, embedding_layer, model_type, X_train, y_train, X_val, y_val, dense)\u001b[0m\n\u001b[0;32m   2093\u001b[0m         '''\n\u001b[0;32m   2094\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2095\u001b[1;33m         \u001b[0mtrack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2096\u001b[0m         \u001b[0mplot_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bilstm'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Dimitri\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ANN1 = 'lstm'\n",
    "ANN2 = 'gru'\n",
    "ANN3 = 'bilstm'\n",
    "ANN4 = 'bigru'\n",
    "ANN5 = 'cnn1'\n",
    "ANN6= 'cnn2'\n",
    "input_data = df.freetext.astype(str)\n",
    "output_data = df.hosp_admit\n",
    "test = word_embeddings(input_data, output_data,ANN1,2,1)\n",
    "test1 = word_embeddings(input_data, output_data,ANN2,2,1)\n",
    "we_evaluation(test[3],test1[3],test[0],test[1],test1[0],test1[1],ANN1,ANN2,test[2],test1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = word_embeddings(input_data, output_data,ANN3,2,1)\n",
    "test1 = word_embeddings(input_data, output_data,ANN4,2,1)\n",
    "we_evaluation(test[3],test1[3],test[0],test[1],test1[0],test1[1],ANN3,ANN4,test[2],test1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = word_embeddings(input_data, output_data,ANN5,2,1)\n",
    "test1 = word_embeddings(input_data, output_data,ANN6,2,1)\n",
    "we_evaluation(test[3],test1[3],test[0],test[1],test1[0],test1[1],ANN5,ANN6,test[2],test1[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
